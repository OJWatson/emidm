{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Training Deep Learning Surrogates for Infectious Disease Modelling\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OJWatson/emidm/blob/main/examples/AIMS_surrogate_notebook.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "**Authors:** Oliver (OJ) Watson Â©. 2025. MIT Licence 2.0. <br>\n",
        "**Affiliation:** MRC-GIDA, School of Public Health, Imperial College London <br>\n",
        "**Contact:** [Email](mailto:o.watson15@imperial.ac.uk) | [Website](https://www.ojwatson.co.uk/) | [Scholar](https://scholar.google.co.uk/citations?user=0jAMBM8AAAAJ&hl=en)\n"
      ],
      "metadata": {
        "id": "kRem1nYgr0yr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Welcome"
      ],
      "metadata": {
        "id": "Y5TukVLduNi7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un43Kdwn4Sf9"
      },
      "source": [
        "This notebook demonstrates how to generate data using a Susceptible-Infected-Recovered (SIR) model with the `emidm` package. The generated data can be used to train deep learning surrogates for infectious disease modeling, which can be used to respond to infectious disease outbreaks.\n",
        "\n",
        "The first section describes an outbreak of influenza in 1978, which should be read closely to understand the domain of the problem we are responding to and how this information can guide the creation of our surrogate model.\n",
        "\n",
        "The second two sections (`Generate Simulation Data` and `Prepare Training Dataset`) make use of `emidm` to generate our training data and are provided in brief to show what data is being generated and to ease into the notebook. The next three sections introduce how to train your surrogate model, validate this and explore ways of using this.\n",
        "\n",
        "The last section are possible extensions for those who finish early, which ask you to think about tweaking the code used to test your understanding about how surrogates are trained, or to explore how to use them further analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Slido Polls and Assessment\n",
        "\n",
        "In the notebook, there are:\n",
        "\n",
        "1. **Embedded Polls**. Answers to these are **not assessed** and can be answered and discussed within your groups. These are used so we can track how far everyone is progressing and to check if it would be helpful to share more material or go over the presentation.\n",
        "\n",
        "  - The polls are a mix of short answers and multiple choice questions\n",
        "  - There is also a Q&A tab within the Slido poll - please feel free to use this to ask questions (our demonstrators won't bite, but sometimes it's easier to ask a question anonymously, which we will then respond to)\n",
        "  - The last Slido poll is a 1-minute paper. This is a very brief feedback form that should only take 1-miute and is aimed at helping me (OJ) to get better. This will be followed by a formal feeback form for the class and demonstrators as a whole\n",
        "\n",
        "2. **Assessed Google Forms**. The assessment for this session will happen fully through an [Assessment Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdLzBlKhREQTFVnxE4M6wANYWw98oAAP2JJ0E_5hoWr6pWk0w/viewform?usp=header). In the notebook, there are call outs to when each section of the form can be tackled based on the material covered in the previous section of the notebook.\n",
        "\n",
        "  - You will only be able to answer the form once fully.\n",
        "  - You can go back and forward throughout the form but do not click Submit until you are happy with your answers.\n",
        "  - The form *should* remember your progress as you have to complete the form with your Google Account. But for safety - once you have opened the form, keep it open in a separate tab.\n",
        "\n",
        "3. **Hints**. There are hints throughout the notebook. Pay attention to these as these will help you when completing the notebook and the assessment."
      ],
      "metadata": {
        "id": "BZfnWi_cdY5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objectives\n",
        "\n",
        "- Fetch data on an influenza outbreak.\n",
        "- Simulate SIR model dynamics using the `emidm` package.\n",
        "- Generate multiple realizations of the model with varying parameters.\n",
        "- Prepare the simulated data for training deep learning surrogates.\n",
        "- Train different surrogate models and compare these\n",
        "- Visualise performance of surrogates"
      ],
      "metadata": {
        "id": "Wjp7UH8xS93N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enabling GPU Acceleration\n",
        "\n",
        "To utilize GPUs for faster training of our neural networks, we need to change the runtime type in your Colab notebook or similar cloud-based environments. Here are the general steps:\n",
        "\n",
        "1. Navigate to Runtime Settings:\n",
        "  - In Colab, go to \"Runtime\" > \"Change runtime type.\"\n",
        "  - In other environments, look for similar options in the settings or configuration menu.\n",
        "\n",
        "2. Select Hardware Accelerator:\n",
        "  - Choose \"GPU\" from the \"Hardware accelerator\" dropdown menu.\n",
        "\n",
        "3. Save and Restart Runtime:\n",
        "  - Click \"Save\" to apply the changes. This will usually restart the runtime environment.\n",
        "\n",
        "4. Verify GPU Availability:\n",
        "  - After the restart, you can run a code snippet like this to confirm that the GPU is recognized and accessible.\n",
        "  - You do not need to do this as we will check this later, but if you want to (and want to practice adding a code cell then plaese do\n",
        "  \n",
        "```python\n",
        "import torch\n",
        "\n",
        "     if torch.cuda.is_available():\n",
        "         print(\"GPU is available!\")\n",
        "         print(f\"Using device: {torch.cuda.get_device_name(0)}\")\n",
        "     else:\n",
        "         print(\"GPU is not available. Using CPU.\")\n",
        "```\n"
      ],
      "metadata": {
        "id": "dhxM3CiKaqep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prerequisites\n",
        "\n",
        "We will be using the helper functions in the `emidm` package, which will be installed and all relevant modules from this as well as other required packages loaded below in two steps.\n",
        "\n",
        "First we will install `emidm` from Github:"
      ],
      "metadata": {
        "id": "_ncHSCL3TA9u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4UlbYJk54Sf9"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/OJWatson/emidm.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will import any required modules once here. If this works then the rest of the notebook should work ðŸ¤ž:"
      ],
      "metadata": {
        "id": "yC5E3jtkTDsi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "XkxysaCH4Sf9"
      },
      "outputs": [],
      "source": [
        "# Imports from our own package\n",
        "from emidm.sir import run_sir, run_model_with_replicates, plot_model_outputs\n",
        "from emidm.plotting import sir_facet_plot, plot_training_histories\n",
        "from emidm.sampler import generate_lhs_samples\n",
        "\n",
        "# imports of other common packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import IPython\n",
        "\n",
        "# for those who like me prescrbe to Hadley Wickham's one truth of a grammar of graphics\n",
        "from plotnine import ggplot, aes, geom_line, facet_wrap, labs, facet_grid, theme_bw\n",
        "\n",
        "# for providing times on notebook for loops\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# Import necessary libraries for neural networks and ML aspect\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Overall Assessment Aim"
      ],
      "metadata": {
        "id": "8sxdQxaoBJ9t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Responding to an outbreak of influenza in school children\n",
        "\n",
        "In 1978, there was a very fast outbreak of influenza A (H1N1) in 1978 at a British boarding school, with the epidemic being tracked by the numbers of students in bed (infectious individuals).\n",
        "\n",
        "In the boarding school, 512 students total were confined to bed within 14 days after the outbreak started on the 22nd of January to the 4th of February. It is reported that one infected boy started the epidemic, which spread rapidly in the relatively closed community of the boarding school.\n",
        "\n",
        "The outbreak was published in the British Medical Journal in 1978 (including hand drawn outbreak curves), and the data is freely available in the R package outbreaks, maintained as part of the [R Epidemics Consortium](https://www.repidemicsconsortium.org/), but we will fetch it from an easier source.\n",
        "\n",
        "![Outbreak Article](https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/outbreak_article.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "1qLSBAHvBeXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch the 1987 flu outbreak data\n",
        "flu_raw = pd.read_csv(\"https://raw.githubusercontent.com/elizavetasemenova/prob-epi/refs/heads/main/data/influenza_england_1978_school.csv\")\n",
        "flu_df = pd.DataFrame({\"t\": flu_raw.iloc[:,0].values-1, \"date\": flu_raw[\"date\"] ,\"I\": flu_raw[\"in_bed\"], \"R\": flu_raw['convalescent']})\n",
        "flu_df.plot(\"t\", [\"I\",\"R\"], xlabel = \"Days After First Cases\", ylabel = \"Number of Infectious Students\")"
      ],
      "metadata": {
        "id": "v1-GZJuOBPHD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a surrogate to respond to the 1978 flu outbreak\n",
        "\n",
        "The main task of this assessment is to generate training data using an SIR infectious disease model that can train a deep learning surrogate suitable to be used for responding to the 1978 flu outbreak.\n",
        "\n",
        "> **Hint**: You will likely need to come back to the description of this dataset to help with later sections of the assessment."
      ],
      "metadata": {
        "id": "_FPvP8jKMbaz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Slido Poll 1: Size of our population {\"vertical-output\":true}\n",
        "# Display the associated webpage in a new window\n",
        "url = 'https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/c6b3a34b-21f1-423b-9d7f-23fc44ce5f15'\n",
        "IPython.display.IFrame(url, width = 700, height = 400)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "VO7qG5ZwZLXL",
        "outputId": "3498c730-1297-4d58-a1ed-9b7946ce4d70",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7b961c7935d0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"700\"\n",
              "            height=\"400\"\n",
              "            src=\"https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/c6b3a34b-21f1-423b-9d7f-23fc44ce5f15\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. The SIR Model\n",
        "\n",
        "In this section, we'll introduce the Susceptible-Infected-Recovered (SIR) model, a fundamental tool for simulating infectious disease dynamics.\n",
        "\n",
        "We'll explore how to run simulations, adjust parameters like transmission rate (Î²) and recovery rate (Î³), and understand the impact of stochasticity by running multiple realizations. We will also introduce the key concept of R<sub>0</sub> and its importance in understanding epidemic behavior.\n",
        "\n",
        ">**Hint:** There is no code to write in this section. However, it provides you an overview of the stochastic SIR model and will give you the opportunity to explore its outputs, which will be helpful in later sections."
      ],
      "metadata": {
        "id": "ZreJpj0wT1EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running a Single SIR Model Simulation\n",
        "\n",
        "We can use `emidm` to simulate the SIR model dynamics using default parameters:"
      ],
      "metadata": {
        "id": "SkMWY8tVT3dm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HcjvbL_c4Sf_"
      },
      "outputs": [],
      "source": [
        "# Demonstrate running one model\n",
        "single = run_sir()\n",
        "\n",
        "# Show the output\n",
        "single"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a `pandas DataFrame` containing the number of susceptible (S), infected (I), and recovered (R) individuals over time.\n",
        "\n",
        "To visualize the results:"
      ],
      "metadata": {
        "id": "WTlczc75T6Kt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82kCqH7o4Sf_"
      },
      "outputs": [],
      "source": [
        "# Show a single plot line\n",
        "single.plot(\"t\", [\"S\", \"I\", \"R\"])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can adjust parameters such as the transmission rate (`beta`), the initial number of infections (`I0`) and total population size (`N`) to observe different dynamics:"
      ],
      "metadata": {
        "id": "C8qW6e8blaBG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbmSSPpD4SgA"
      },
      "outputs": [],
      "source": [
        "# We can also vary the parameters\n",
        "alt = run_sir(beta = 1.6, I0 = 2, N = 500)\n",
        "alt.plot(\"t\", [\"S\", \"I\", \"R\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Multiple Stochastic Realisations\n",
        "\n",
        "To account for stochasticity, we can run multiple realizations of the model:"
      ],
      "metadata": {
        "id": "N8IgN7peT_F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we can run multiple realisations\n",
        "reps = run_model_with_replicates(model = run_sir, reps = 10)\n",
        "reps"
      ],
      "metadata": {
        "id": "oaQjv6jrg5qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sfz6cXYf4SgA"
      },
      "outputs": [],
      "source": [
        "# we can run multiple realisations\n",
        "reps = run_model_with_replicates(model = run_sir, reps = 10)\n",
        "\n",
        "# and plot these\n",
        "p = plot_model_outputs(reps)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And we can also pass through any of the arguments to `run_sir` to our `run_model_with_replicates` function."
      ],
      "metadata": {
        "id": "3FzG0e29UBy9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jyEta3T4SgA"
      },
      "outputs": [],
      "source": [
        "# we can also by args to run_sir through kwargs\n",
        "reps = run_model_with_replicates(model=run_sir, reps=10, beta = 0.3)\n",
        "\n",
        "# and plot these\n",
        "p = plot_model_outputs(reps, columns = [\"I\", \"R\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Slido Poll 2: Parameters in SIR Model {\"vertical-output\":false}\n",
        "# Display the associated webpage in a new window\n",
        "url = 'https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/f1d6e04f-76dc-43ac-9b46-a027d0194b11'\n",
        "IPython.display.IFrame(url, width = 700, height = 400)"
      ],
      "metadata": {
        "id": "7NNjfWx7bt1T",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "outputId": "951a60b8-3845-4366-8a62-95bf37a4c2ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x79fe7c1b5d50>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"700\"\n",
              "            height=\"400\"\n",
              "            src=\"https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/f1d6e04f-76dc-43ac-9b46-a027d0194b11\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Understanding R<sub>0</sub>\n",
        "\n",
        "R<sub>0</sub>, pronounced \"R-naught,\" is a fundamental concept in epidemiology. It represents the **average number of secondary infections caused by a single infected individual** in a completely susceptible population.\n",
        "\n",
        "In simpler terms: If one person has the disease, R<sub>0</sub> tells us how many other people they are likely to infect.\n",
        "\n",
        "**For the SIR model used in this notebook, R<sub>0</sub> is defined as:**\n",
        "\n",
        "R<sub>0</sub> = Î² / Î³\n",
        "\n",
        "where:\n",
        "\n",
        "* Î² (beta) is the transmission rate\n",
        "* Î³ (gamma) is the recovery rate\n",
        "\n",
        "**Exploring R<sub>0</sub> = 1:**\n",
        "\n",
        "When R<sub>0</sub> is equal to 1, it means each infected person infects, on average, one other person. This represents a critical threshold:\n",
        "\n",
        "* **R<sub>0</sub> > 1:** The epidemic will grow.\n",
        "* **R<sub>0</sub> < 1:** The epidemic will decline and eventually die out.\n",
        "* **R<sub>0</sub> = 1:** The epidemic will remain stable, with the number of infected individuals neither increasing nor decreasing significantly.\n",
        "\n",
        "**Let's investigate the dynamics of an epidemic when R<sub>0</sub> is equal to 1.**"
      ],
      "metadata": {
        "id": "DFTQcZMYl3y8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title R0 Exploration  {\"vertical-output\":false}\n",
        "\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from emidm.sir import run_sir, run_model_with_replicates, plot_model_outputs\n",
        "\n",
        "# Define widgets for user input\n",
        "beta_slider = widgets.FloatSlider(value=0.6, min=0.1, max=2.0, step=0.1, description='Beta:')\n",
        "gamma_slider = widgets.FloatSlider(value=0.3, min=0.1, max=1.0, step=0.05, description='Gamma:')\n",
        "N_slider = widgets.IntSlider(value=1000, min=100, max=10000, step=100, description='N:')\n",
        "I_slider = widgets.IntSlider(value=10, min=1, max=20, step=1, description='I0:')\n",
        "reps_slider = widgets.IntSlider(value=20, min=1, max=20, step=1, description='Reps:')\n",
        "t_slider = widgets.IntSlider(value=100, min=50, max=300, step=10, description='Time (days):')\n",
        "\n",
        "# Output widget to display the plot and R0\n",
        "output = widgets.Output()\n",
        "\n",
        "def update_plot(beta, gamma, N, reps, time, I0):\n",
        "    with output:\n",
        "        clear_output(wait=True)\n",
        "        reps_data = run_model_with_replicates(model=run_sir, reps=reps, beta=beta, gamma=gamma, N=N, T=time, I0 = I0)\n",
        "        r0 = beta / gamma\n",
        "        p = plot_model_outputs(reps_data, show = False)\n",
        "        p = (p + labs(title = (f\"SIR Model Dynamics (R0 = {r0:.2f})\")))  # Format R0 to 2 decimal places\n",
        "        p.show()\n",
        "\n",
        "# Interactive function to update the plot\n",
        "def on_value_change(change):\n",
        "    update_plot(beta_slider.value, gamma_slider.value, N_slider.value, reps_slider.value, t_slider.value, I_slider.value)\n",
        "\n",
        "# Observe changes in widget values\n",
        "beta_slider.observe(on_value_change, names='value')\n",
        "gamma_slider.observe(on_value_change, names='value')\n",
        "N_slider.observe(on_value_change, names='value')\n",
        "reps_slider.observe(on_value_change, names='value')\n",
        "t_slider.observe(on_value_change, names='value')\n",
        "I_slider.observe(on_value_change, names='value')\n",
        "\n",
        "# Arrange sliders in two rows using HBox and VBox\n",
        "row1 = widgets.HBox([N_slider, reps_slider, t_slider])\n",
        "row2 = widgets.HBox([beta_slider, gamma_slider, I_slider])\n",
        "ui = widgets.VBox([row1, row2])\n",
        "\n",
        "# Display widgets and output\n",
        "display(ui, output)\n",
        "\n",
        "# Initial plot\n",
        "update_plot(beta_slider.value, gamma_slider.value, N_slider.value, reps_slider.value, t_slider.value, I_slider.value)\n"
      ],
      "metadata": {
        "id": "Gjn1r35JpC37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Prepare Training Dataset\n",
        "\n",
        "\n",
        "In the previous section we introduced a infectious disase model for simulating SIR dynamics. The model was fairly simple and runs quickly even for a large number of replicates. Because the model is simple, we could use this quickly to fit to the school outbreak by with brute force (e.g. grid search by exploring ranges of beta and gamma) or by using it in particle MCMC.\n",
        "\n",
        "**However**, let's imagine, that this model took hours or even days to do one stochastic realisation. Suddenly we are unable to use this model as easily. This is where the surrogate model would be helpful to train.\n",
        "\n",
        "However, to train the surrogate, we first need to generate training data.\n",
        "\n",
        "> **Hint:** Remember, the surrogate is only useful if the training data it was trained on reflects the situation we or dataset we hope to use it for, i.e. for the 1978 flu outbreak."
      ],
      "metadata": {
        "id": "bBkpytKbUGMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling Parameter Space with Latin Hypercube Sampling\n",
        "\n",
        "To systematically explore the parameter space, we use Latin Hypercube Sampling (LHS), which we have again provided helper functions from `emidm` for you to use.\n"
      ],
      "metadata": {
        "id": "xk0GVw0qUHbz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we specify an upper and lower for each parameter we want to sample\n",
        "param_ranges = {\"beta\": [0.1, 0.4], \"gamma\": [0.05, 0.3], \"I0\": [1, 500]}\n",
        "df_samples = generate_lhs_samples(param_ranges, n_samples=9, seed=42)\n",
        "df_samples"
      ],
      "metadata": {
        "id": "d6RelyTR_iRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This generates a set of parameter combinations, which we can then pass to our `run_model_with_replicates` function. We have just used 9 samples here initially just to show you the outputs and understand it. Later we will generate more samples to build a robust training dataset.\n",
        "\n",
        "However, before that we need to turn our `I0` column into integers.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/add_code.png\" alt=\"Add Code 1\" width=\"200\"/>\n"
      ],
      "metadata": {
        "id": "aJNR_Yd-UKjt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R6BzMV4V4SgD"
      },
      "outputs": [],
      "source": [
        "# ADD YOUR CODE HERE\n",
        "df_samples[\"I0\"] = # insert code here\n",
        "\n",
        "# Run the model for each row of samples:\n",
        "results = [\n",
        "    run_model_with_replicates(**row.to_dict(), reps=10).assign(**row.to_dict())\n",
        "    for _, row in df_samples.iterrows()\n",
        "]\n",
        "\n",
        "# Combine results into one DataFrame:\n",
        "df_all_results = pd.concat(results, axis=0)\n",
        "\n",
        "# Make a facet plot to see the different simulations\n",
        "plot_facet = sir_facet_plot(df_all_results)\n",
        "plot_facet"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generating Training and Test Data\n",
        "\n",
        "Now that we have seen how LHS is being used to sample different $\\beta$ and $\\gamma$ parameters and to generate simulations, we will now generate training, validation and test data in much the same way.\n",
        "\n",
        "**HOWEVER** in the next section you will need to provide your own code for what parameter ranges to sample.\n",
        "\n",
        "> **Hint**: Think about what ranges to specify to generate training data tailored for our problem of fitting a model to the 1978 epidemic.\n"
      ],
      "metadata": {
        "id": "dsDjrHU-UOaL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<img src=\"https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/add_code.png\" alt=\"Add Code 1\" width=\"200\"/>\n"
      ],
      "metadata": {
        "id": "W7wLDb67k6i4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOOoy_w54SgA"
      },
      "outputs": [],
      "source": [
        "# now to generate a lhs sample based on R0 and gamma\n",
        "def beta_gamma_from_r0_gamma(n_samples, param_ranges, seed = None):\n",
        "    df_samples = generate_lhs_samples(param_ranges, n_samples=n_samples, seed=seed)\n",
        "    df_samples.insert(1, \"beta\", df_samples[\"R0\"] * df_samples[\"gamma\"])\n",
        "    df_samples = df_samples.drop(columns=[\"R0\"])\n",
        "    return df_samples\n",
        "\n",
        "# Insert Code Here for your Parameter Ranges\n",
        "R0_lower = # insert code here\n",
        "R0_upper = # insert code here\n",
        "gamma_lower = # insert code here\n",
        "gamma_higher = # insert code here\n",
        "I0_lower = # insert code here\n",
        "I0_higher = # insert code here\n",
        "\n",
        "# you can use the code below to check what parameter sets these ranges will yield\n",
        "param_ranges = {\"R0\": [R0_lower, R0_upper],\n",
        "                \"gamma\": [gamma_lower, gamma_higher],\n",
        "                \"I0\": [I0_lower, I0_higher]}\n",
        "df_samples = beta_gamma_from_r0_gamma(n_samples=9, seed=42, param_ranges = param_ranges)\n",
        "df_samples"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# now to generate a lhs sample based on R0 and gamma\n",
        "def beta_gamma_from_r0_gamma(n_samples, param_ranges, seed = None):\n",
        "    df_samples = generate_lhs_samples(param_ranges, n_samples=n_samples, seed=seed)\n",
        "    df_samples.insert(1, \"beta\", df_samples[\"R0\"] * df_samples[\"gamma\"])\n",
        "    df_samples = df_samples.drop(columns=[\"R0\"])\n",
        "    return df_samples\n",
        "\n",
        "param_ranges = {\"R0\": [1.1, 4], \"gamma\": [0.05, 0.25], \"I0\": [1,11]}\n",
        "\n",
        "df_all_results"
      ],
      "metadata": {
        "id": "ozz3vwG315z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you have decided on your parameter ranges to use for generating your training data, you can use the code below to actually generate your training data by running multiple simulations. Make sure you are happy with your chosen `param_ranges`, as the code below will take a few minutes to run.\n",
        "\n",
        "> **Hint**: The number of parameter sets to sample to generate your training and validation data (`n_train`, `n_valid`) as well as the number of stochastic realisations to simulate (`reps`) for each parameter set is already set below. This is enough to train an effective surrogate **if** your `param_ranges` were well chosen."
      ],
      "metadata": {
        "id": "UWpimw3axc-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Number of parameter sets to generate\n",
        "n_train = 1000\n",
        "n_valid = 200\n",
        "n_test = 100\n",
        "\n",
        "# Number of stochastic realisations for each parameter set\n",
        "reps = 5\n",
        "\n",
        "# generate train samples and data\n",
        "train_samples = beta_gamma_from_r0_gamma(n_samples=n_train, param_ranges = param_ranges)\n",
        "train_data = pd.concat([\n",
        "    run_model_with_replicates(**row.to_dict(), reps=reps).assign(**row.to_dict())\n",
        "    for _, row in tqdm(train_samples.iterrows(), total=len(train_samples))\n",
        "], axis = 0)\n",
        "\n",
        "# generate valid samples\n",
        "valid_samples = beta_gamma_from_r0_gamma(n_samples=n_valid, param_ranges = param_ranges)\n",
        "valid_data = pd.concat([\n",
        "    run_model_with_replicates(**row.to_dict(), reps=reps).assign(**row.to_dict())\n",
        "    for _, row in tqdm(valid_samples.iterrows(), total = len(valid_samples))\n",
        "], axis = 0)\n",
        "\n",
        "# generate test samples\n",
        "test_samples = beta_gamma_from_r0_gamma(n_samples=n_test, param_ranges = param_ranges)\n",
        "test_data = pd.concat([\n",
        "    run_model_with_replicates(**row.to_dict(), reps=reps).assign(**row.to_dict())\n",
        "    for _, row in tqdm(test_samples.iterrows(), total = len(test_samples))\n",
        "], axis = 0)"
      ],
      "metadata": {
        "id": "P_3Ka4xoUBJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessment 1."
      ],
      "metadata": {
        "id": "91s9NJ_9tc5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please head to the [Assessment Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdLzBlKhREQTFVnxE4M6wANYWw98oAAP2JJ0E_5hoWr6pWk0w/viewform?usp=header) and complete Section 1.\n",
        "\n",
        "**Keep your Google Form open in a tab once you have completed section 1**\n",
        "\n",
        "> Hint: To answer the questions you may need to write new cells with code to generate plots to answer some of the questions. To save these plots from Colab, Right Click > Save Image As\n",
        "\n",
        "![Assessment 1](https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/assessment_1.png)"
      ],
      "metadata": {
        "id": "0hGxPe63JD6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training our surrogate\n",
        "\n",
        "In this section, we will focus on training our deep learning surrogate model. This involves preparing the data, constructing different neural network architectures.\n",
        "\n",
        "We have provided the `Dataset` class for you and some deep learning architectures, but you will be required to write your own as well."
      ],
      "metadata": {
        "id": "9IeF0J1GUTfy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating Dataset Class and Dataloader\n",
        "\n",
        "While we have generated our training and test data, we need to prepare it into a PyTorch dataset and dataloader. The dataloader is a way of iterating through our data in batches, which is useful for training deep learning models. Batches are used to train the model in mini-batches, which is more efficient than training on the entire dataset at once. It also has the advantage of allowing us to use GPU acceleration if available.\n",
        "\n",
        "We have not created this dataset class yet in `emidm`, so that you can see you how it works more easily, which will help you in writing your own surrogate model later."
      ],
      "metadata": {
        "id": "4jegHgEAUU1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SIRTimeSeriesDataset(Dataset):\n",
        "    def __init__(self, dataframe, features=['beta', 'gamma', \"I0\"], targets=['S', 'I']):\n",
        "        self.features = features\n",
        "        self.targets = targets\n",
        "\n",
        "        # Group by parameter sets\n",
        "        self.grouped = dataframe.groupby(features + ['replicate'])\n",
        "        self.param_sets = list(self.grouped.groups.keys())\n",
        "        self.dataframe = dataframe\n",
        "\n",
        "        # Extract timepoints (assuming they're the same for all parameter sets)\n",
        "        self.timepoints = dataframe['t'].unique()\n",
        "        self.time_length = len(self.timepoints)\n",
        "        self.N = dataframe['N'].iloc[0]\n",
        "\n",
        "        # Create samples\n",
        "        self.samples = []\n",
        "        for param_set in self.param_sets:\n",
        "            df_group = self.grouped.get_group(param_set).sort_values('t')\n",
        "\n",
        "            # Extract feature and target time series\n",
        "            feature_values = np.array([param_set[features.index('beta')],\n",
        "                                      param_set[features.index('gamma')],\n",
        "                                      param_set[features.index('I0')]])\n",
        "\n",
        "            # Extract full target time series\n",
        "            target_series = np.column_stack([df_group[target].values / self.N for target in targets])\n",
        "\n",
        "            # Store full time series with parameter info\n",
        "            self.samples.append({\n",
        "                'features': feature_values,\n",
        "                'targets': target_series,\n",
        "                'param_values': param_set\n",
        "            })\n",
        "\n",
        "        # Normalize input features\n",
        "        feature_array = np.array([s['features'] for s in self.samples])\n",
        "        self.feature_scaler = StandardScaler()\n",
        "        self.feature_scaler.fit(feature_array)\n",
        "\n",
        "        # No need to normalize S and I as they're already in [0,1] range\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.samples[idx]\n",
        "\n",
        "        # Normalize features\n",
        "        features_normalized = self.feature_scaler.transform(sample['features'].reshape(1, -1)).flatten()\n",
        "\n",
        "        return torch.tensor(features_normalized, dtype=torch.float32), \\\n",
        "               torch.tensor(sample['targets'], dtype=torch.float32)"
      ],
      "metadata": {
        "id": "eWbSC3lgUCBZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Slido Poll 3: Output Normalisation {\"vertical-output\":false}\n",
        "# Display the associated webpage in a new window\n",
        "url = 'https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/45869e34-e663-4cb3-b336-41b86f7741e1'\n",
        "IPython.display.IFrame(url, width = 700, height = 400)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "OmZtnzAm6vbr",
        "outputId": "5bd8de6e-712e-4594-fabe-df63f04c4a37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7b9515752190>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"700\"\n",
              "            height=\"400\"\n",
              "            src=\"https://app.sli.do/event/1HnwbEVFT7HATeDCdCHtHL/embed/polls/45869e34-e663-4cb3-b336-41b86f7741e1\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having made our `SIRTimeSeriesDataset` class, we can use this to create a `SIRTimeSeriesDataset` object for each of our different data sets (train, validation, test).\n",
        "\n",
        "After this we then create `DataLoaders`. This is a PyTorch class used to efficiently iterate through the datasets during training. It handles tasks like batching, shuffling (for the training set), and potentially loading data in parallel.\n"
      ],
      "metadata": {
        "id": "SB41pfWMUhEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = SIRTimeSeriesDataset(train_data)\n",
        "val_dataset = SIRTimeSeriesDataset(valid_data)\n",
        "test_dataset = SIRTimeSeriesDataset(test_data)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "OtcV8qgLUGWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Batch Size Explained\n",
        "\n",
        "<details>\n",
        "<summary><b>Click to expand</b></summary>\n",
        "\n",
        "In the code above, `batch_size` is set to 64. This means that during training, the model will see 64 samples of data before updating its internal parameters (weights and biases). This process of seeing a batch of data and then updating is called one **iteration**.\n",
        "\n",
        "Instead of training on the entire dataset at once (which can be computationally expensive and memory-intensive), the data is divided into smaller batches. The model iterates through these batches, making adjustments to its parameters after seeing each batch.\n",
        "\n",
        "##### *Why Use Batches?*\n",
        "\n",
        "**1. Computational Efficiency:** Processing the entire dataset in one go can be very slow, especially for large datasets. Batches make the process more manageable by breaking it down into smaller steps. This is crucial for training deep learning models which often require vast amounts of data.\n",
        "\n",
        "**2. Memory Management:** Loading the entire dataset into memory might not be feasible, especially when dealing with very large datasets or limited hardware resources. Batching allows the model to work with a smaller subset of the data at any given time, reducing memory requirements.\n",
        "\n",
        "**3. Generalization:** Training on batches can improve the model's ability to generalize to unseen data. This is because the model is exposed to a variety of data points in each batch, preventing it from overfitting to specific examples in the training set. Updates based on a single data point at a time (e.g., with a batch size of 1 which is called stochastic gradient descent) could update the weights in a way that is not good for the model's prediction performance over all of the data. Using batches provides a better average for what data the model tends to see and thus better update performance.\n",
        "\n",
        "**4. Noise Reduction:** The gradients (directions for updating model parameters) calculated on a batch are less noisy compared to those calculated on individual samples. This leads to more stable and smoother training, potentially helping the model converge faster to a good solution.\n",
        "\n",
        "##### *Choosing the Right Batch Size*\n",
        "\n",
        "The choice of batch size is a hyperparameter that can significantly impact the training process. There's no one-size-fits-all answer, and the optimal batch size often depends on factors like:\n",
        "\n",
        "**1. Dataset Size:** Larger datasets can handle larger batch sizes, while smaller datasets might require smaller batches to avoid overfitting.\n",
        "**2. Model Architecture:** Complex models with many parameters might benefit from larger batch sizes for better gradient estimations.\n",
        "**3. Hardware:** Available memory and processing power influence the maximum batch size you can use.\n",
        "**4. Training Time:** Larger batches can lead to faster training epochs (one pass through the entire dataset), but they might require more epochs to converge. Smaller batches require more iterations per epoch and are usually slower but are helpful in preventing issues like overfitting.\n",
        "\n",
        "##### *In Practice:*\n",
        "\n",
        "- Common Batch Sizes: 32, 64, 128, 256 are frequently used batch sizes.\n",
        "- Experimentation: It's often necessary to experiment with different batch sizes to find the best one for a particular problem. You can evaluate performance on a validation set to guide your choice.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "_sGtVml_-O6-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Neural Network Models\n",
        "\n",
        "This section of the code defines and trains different types of neural networks to predict the Susceptible (S) and Infected (I) populations in the SIR model given the input parameters beta, gamma and `I0`.\n",
        "\n",
        "Before we delve into the specific model types, let's define some key terminology in neural networks:\n",
        "\n",
        "- **Input Size:** The number of input features to the network. In this case, it's 3, representing beta, gamma and `I0`.\n",
        "\n",
        "- **Hidden Size:** The number of neurons in the hidden layers of the network. This controls the complexity and capacity of the model. A larger hidden size means the model can learn more complex patterns but might be prone to overfitting. Here, hidden_size is set to 64.\n",
        "\n",
        "- **Output Size:** The number of output values the network produces. Here, it's 2, corresponding to the predicted S and I values.\n",
        "\n",
        "- **Layers:** Neural networks consist of interconnected layers of neurons. Hidden layers process the input data and extract features, while the output layer produces the final predictions. Deeper networks (more hidden layers) can learn more complex relationships, but they are also more computationally intensive to train. This code uses hidden layers.\n",
        "\n",
        "- **Dropout:** A regularization technique that helps prevent overfitting. During training, dropout randomly ignores a fraction of the neurons in a layer, forcing the network to learn more robust features that are not dependent on any single neuron. dropout_prob (set to 0.1 here) controls the probability of a neuron being dropped out."
      ],
      "metadata": {
        "id": "2FhsK66PWJy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define constants\n",
        "input_size = 3  # beta and gamma and I0\n",
        "hidden_size = 64\n",
        "output_size = 2  # S and I\n",
        "num_layers = 2\n",
        "dropout_prob = 0.1"
      ],
      "metadata": {
        "id": "1_HwYBpOUK5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Feedforward Neural Network (FFNN)\n",
        "\n",
        "This is the simplest type of neural network, where data flows in one direction from input to output. It has multiple layers, each containing a number of neurons (or units). It uses dropout for regularization, with a probability defined by dropout_prob that a neuron is ignored in a given step. It also has batch normalization and ReLU activation function for each hidden layer.\n",
        "\n",
        "<details>\n",
        "<summary><b>Click to read more</b></summary>\n",
        "\n",
        "In our case, The FFNN model takes $\\beta$ and $\\gamma$ and `I0` as input and aims to predict the time series of `S` and `I` values. Here's a simplified breakdown of how it works:\n",
        "\n",
        "- **Input:** The input values (beta and gamma and `I0`) are fed into the first layer of the network.\n",
        "\n",
        "- **Hidden Layers:** The input values are processed through a series of hidden layers. Each layer consists of neurons that perform calculations on the input data using weights and biases.\n",
        "  - The output of each neuron is passed through an activation function (ReLU here), which introduces non-linearity to the model.\n",
        "  - Dropout is applied within the hidden layers to prevent overfitting.\n",
        "  - Batch normalization is used to improve training stability and performance by normalizing the inputs to each layer.\n",
        "\n",
        "- **Output Layer:** The final hidden layer's output is fed to the output layer, which produces the predictions for `S` and `I`. The output uses a sigmoid activation function, ensuring output values are between 0 and 1, representing the proportion of the population.\n",
        "\n",
        "- **Training:** During training, the network's weights and biases are adjusted to minimize the difference between the predicted and actual `S` and `I` values (the loss). This adjustment is done using an optimization algorithm like Adam.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "pM6eLSO9W7cv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feedforward Neural Network\n",
        "class FFNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, time_steps, output_channels):\n",
        "        super(FFNN, self).__init__()\n",
        "        self.time_steps = time_steps\n",
        "        self.output_channels = output_channels\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Deeper network with dropout and batch normalization\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.BatchNorm1d(hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "\n",
        "            nn.Linear(hidden_size, hidden_size*2),\n",
        "            nn.BatchNorm1d(hidden_size*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "\n",
        "            nn.Linear(hidden_size*2, hidden_size*2),\n",
        "            nn.BatchNorm1d(hidden_size*2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "\n",
        "            nn.Linear(hidden_size*2, time_steps * output_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Output shape: [batch_size, time_steps, output_channels]\n",
        "        output = self.network(x)\n",
        "        output = output.view(-1, self.time_steps, self.output_channels)\n",
        "        output = self.sigmoid(output)  # Apply sigmoid\n",
        "        return output"
      ],
      "metadata": {
        "id": "F1o37pIsW57K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Gated Recurrent Unit (GRU)\n",
        "\n",
        "This is a type of recurrent neural network (RNN) that is designed to handle sequential data, like the time series data from the SIR model. GRUs have a special mechanism called \"gates\" that help them learn long-term dependencies in the data.\n",
        "\n",
        "<details>\n",
        "<summary><b>Click to read more</b></summary>\n",
        "\n",
        "The key features of GRUs are:\n",
        "\n",
        "- **Update Gate:** Decides how much past information to keep and how much to update.\n",
        "- **Reset Gate:** Controls how much past information to ignore when computing the new state.\n",
        "- **Hidden State:** Represents the current input and relevant past information.\n",
        "\n",
        "**How it Works Here:**\n",
        "\n",
        "- Inside the `forward` method of the `GRUModel`, this input vector is projected to a higher-dimensional space and then repeated for each time step to create a sequence (`x_seq`). This effectively creates a constant input sequence where the same `beta` and `gamma` values are presented to the GRU at each time step.\n",
        "\n",
        "- The GRU layer then processes this input sequence, but its primary focus is on learning the temporal relationships within the output sequence (`S` and `I` over time). The hidden state of the GRU evolves based on both the input and the previous hidden state, capturing the dynamic changes in `S` and `I` as the SIR model progresses.\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "BX35pZOOXusI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRU Model\n",
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_prob, num_layers=2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "        # Project parameters to a sequence\n",
        "        self.input_projection = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # GRU layer\n",
        "        self.gru = nn.GRU(\n",
        "            hidden_size, hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout_prob if num_layers > 1 else 0.0,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Output layers\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "        self.ln = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_prob)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        time_steps = train_dataset.time_length\n",
        "\n",
        "        # Create a sequence from the parameter input\n",
        "        x_seq = self.input_projection(x).unsqueeze(1).repeat(1, time_steps, 1)\n",
        "\n",
        "        # Process with GRU\n",
        "        out, _ = self.gru(x_seq)\n",
        "        out = self.ln(out)\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc(out)\n",
        "\n",
        "        # Apply sigmoid activation\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "qbUI47WkXuc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Long Short-Term Memory (LSTM)\n",
        "This is another type of RNN, closely related to GRUs, also designed to handle sequential data like the time series from the SIR model. LSTMs use a more complex gating mechanism than GRUs, involving three gates (input, forget, output) and a cell state, enabling them to learn long-term dependencies and handle vanishing gradients effectively.\n",
        "\n",
        "<details>\n",
        "<summary><b>Click to read more</b></summary>\n",
        "\n",
        "The key features of LSTMs are:\n",
        "\n",
        "- **Input Gate:** Regulates the flow of new information into the cell state.\n",
        "- **Forget Gate:** Controls what information to discard from the cell state.\n",
        "- **Output Gate:** Determines what information from the cell state is outputted.\n",
        "- **Cell State:** Acts as a memory unit, storing and carrying information across time steps.\n",
        "- **Hidden State:** Represents the current output and is influenced by the cell state and the output gate.\n",
        "\n",
        "**How it Works Here:**\n",
        "\n",
        "Similar to the GRU, inside the forward method of the `LSTMModel`, the input vector is projected to a higher-dimensional space and repeated for each time step to form a sequence (`x_seq`). This creates a constant input sequence where the same beta and gamma values are presented to the LSTM at each time step.\n",
        "\n",
        "The LSTM layer processes this input sequence, but its primary focus is on learning the temporal relationships within the output sequence (`S` and `I` over time). Using its three gates and the cell state, the LSTM carefully controls the flow of information, allowing it to selectively remember and forget relevant parts of the past while updating its hidden state to capture the dynamic changes in `S` and `I` as the SIR model progresses.\n",
        "\n",
        "</details>\n",
        "\n",
        "---\n",
        "\n",
        "Below you will need to complete the `LSTMModel` class\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/add_code.png\" alt=\"Add Code 1\" width=\"200\"/>\n"
      ],
      "metadata": {
        "id": "iSlagZCJZq6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, dropout_prob, num_layers=2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "\n",
        "        # Add you code here by assigning the hidden size, layers, sigmoid af\n",
        "        self.hidden_size = # Add your code here\n",
        "        self.num_layers = # Add your code here\n",
        "        self.sigmoid = # Add your code here\n",
        "\n",
        "        # Project parameters to a sequence\n",
        "        self.input_projection = # Add your code here\n",
        "\n",
        "        # LSTM layer - we can use nn.LSTM\n",
        "        self.lstm = # Add your code here\n",
        "\n",
        "        # Output layers\n",
        "        self.fc = # Add your code here\n",
        "        self.ln = # Add your code here\n",
        "        self.dropout # Add your code here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Complete the forward step function\n",
        "\n",
        "        # Create batch size and time steps\n",
        "\n",
        "        # Create a sequence from the parameter input\n",
        "\n",
        "        # Process with LSTM\n",
        "\n",
        "        # Apply sigmoid activation\n",
        "\n",
        "        return out\n"
      ],
      "metadata": {
        "id": "ftVoNh_vZrWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating and Training Our Models\n",
        "\n",
        "Now that we have created our NN classes, we can initialise these models, and determine whether a GPU is available for computation (it should be if you changed your Runtime earlier).\n",
        "\n",
        "If a GPU is found (`torch.cuda.is_available()` returns `True`), it sets the device to 'cuda' (indicating GPU usage); otherwise, it defaults to 'cpu' for CPU-based calculations."
      ],
      "metadata": {
        "id": "49k-AkaeaK_U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize models\n",
        "ffnn_model = FFNN(input_size, hidden_size, train_dataset.time_length, output_size)\n",
        "gru_model = GRUModel(input_size, hidden_size, output_size, dropout_prob, num_layers)\n",
        "lstm_model = LSTMModel(input_size, hidden_size, output_size, dropout_prob, num_layers)\n",
        "\n",
        "# Move models to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "ffnn_model = ffnn_model.to(device)\n",
        "gru_model = gru_model.to(device)\n",
        "lstm_model = lstm_model.to(device)\n",
        "\n",
        "print(f\"Models initialized and moved to {device}\")"
      ],
      "metadata": {
        "id": "3cI0eIfdaLTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Model Training\n",
        "\n",
        "Below, we have provided the `train_model` function, which trains the neural network models. We have provided the steps in full but in practice, you would make use of a number of helpful packages e.g. [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/) to help organizes your PyTorch code, remove boilerplate code and unlock scalability.\n",
        "\n",
        "<details>\n",
        "<summary><b>Click to read more on the training steps</b></summary>\n",
        "\n",
        "1. **Initialization:**\n",
        "    - Defines the loss function (`nn.MSELoss` for Mean Squared Error).\n",
        "    - Sets up the optimizer (`torch.optim.Adam`) to update model parameters.\n",
        "    - Creates a learning rate scheduler (`ReduceLROnPlateau`) to adjust the learning rate by a factor (here, 0.5) when the validation stops improving for a certain number of epochs (here, `patience=5`).\n",
        "    - Early stopping criterria are also set, so if there is no improvement in validation loss for a number of epochs (here, `patience=10`), then training is stopped earlier.\n",
        "\n",
        "2. **Training Loop:**\n",
        "    - Iterates through a specified number of epochs.\n",
        "    - **Training Phase:**\n",
        "        - The model is set to training mode (`model.train()`).\n",
        "        - Processes batches of training data, making predictions and calculating the loss:\n",
        "          - *Forward Pass:* The model makes predictions (outputs) based on the input data (inputs).\n",
        "          - *Calculate Loss:* The loss function (criterion) is used to calculate the error between the predictions (outputs) and the actual target values (targets)\n",
        "          - *Backward Pass and Optimise:* `optimizer.zero_grad()` resets the gradients of the model's parameters. `loss.backward()` calculates the gradients of the loss with respect to the model's parameters. `optimizer.step()` updates the model's parameters based on the calculated gradients and the learning rate.\n",
        "        - Tracks the average training loss for the epoch.\n",
        "    - **Validation Phase:**\n",
        "        - The model is set to evaluation mode (`model.eval()`).\n",
        "        - Processes batches of validation data, making predictions and calculating the validation loss (without updating model parameters).\n",
        "        - Tracks the average validation loss for the epoch.\n",
        "\n",
        "3. **Model Saving and Early Stopping:**\n",
        "    - Saves the best model (lowest validation loss) during training.\n",
        "    - Implements early stopping to prevent overfitting if the validation loss doesn't improve for a certain number of epochs.\n",
        "\n",
        "4. **Returning Results:**\n",
        "    - Returns the trained model and a history dictionary containing training and validation losses, epochs, and information about the best model.\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "uzuD3uHocH7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, lr, model_name, device, patience=10):\n",
        "    # Initialize criterion, optimizer and scheduler\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
        "\n",
        "    # For tracking loss and best model\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_val_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    best_epoch = 0\n",
        "\n",
        "    # Path to save best model\n",
        "    best_model_path = os.path.join(output_dir, f\"{model_name}_best.pt\")\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "\n",
        "        for inputs, targets in train_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_train_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate average training loss\n",
        "        avg_train_loss = total_train_loss / len(train_loader.dataset)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in val_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = criterion(outputs, targets)\n",
        "                total_val_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        # Calculate average validation loss\n",
        "        avg_val_loss = total_val_loss / len(val_loader.dataset)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        # Update scheduler\n",
        "        scheduler.step(avg_val_loss)\n",
        "\n",
        "        # Print progress every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}/{epochs}, {model_name} - '\n",
        "                  f'Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
        "\n",
        "        # Check if this is the best model so far\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            # Save the best model\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': avg_val_loss,\n",
        "                'train_loss': avg_train_loss\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch} with validation loss: {avg_val_loss:.6f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Validation loss did not improve. Patience: {patience_counter}/{patience}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch} epochs. Best was epoch {best_epoch}.\")\n",
        "            break\n",
        "\n",
        "    # Save training history\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'val_loss': val_losses,\n",
        "        'epochs': list(range(1, len(train_losses) + 1)),\n",
        "        'best_epoch': best_epoch,\n",
        "        'best_val_loss': best_val_loss\n",
        "    }\n",
        "\n",
        "    with open(os.path.join(output_dir, f\"{model_name}_history.json\"), 'w') as f:\n",
        "        json.dump(history, f)\n",
        "\n",
        "    # Load the best model\n",
        "    checkpoint = torch.load(best_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return model, history\n",
        "\n"
      ],
      "metadata": {
        "id": "SA3OJUZdUN3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now actually train our models!"
      ],
      "metadata": {
        "id": "lEIQddoVeNXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set training parameters\n",
        "learning_rate = 1e-3\n",
        "num_epochs = 100\n",
        "patience = 10\n",
        "\n",
        "# Create output directory for saving models and plots\n",
        "output_dir = \"emulator_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"Training FFNN model...\")\n",
        "ffnn_model, ffnn_history = train_model(\n",
        "    ffnn_model, train_loader, val_loader, num_epochs, learning_rate, \"ffnn\", device, patience\n",
        ")\n",
        "\n",
        "print(\"Training GRU model...\")\n",
        "gru_model, gru_history = train_model(\n",
        "    gru_model, train_loader, val_loader, num_epochs, learning_rate, \"gru\", device, patience\n",
        ")\n",
        "\n",
        "print(\"Training LSTM model...\")\n",
        "lstm_model, lstm_history = train_model(\n",
        "    lstm_model, train_loader, val_loader, num_epochs, learning_rate, \"lstm\", device, patience\n",
        ")"
      ],
      "metadata": {
        "id": "XCA7d6WVcFch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Training History\n",
        "\n",
        "Below we visualize the training history of three different neural network models (FFNN, GRU, and LSTM), showing the training loss and validation loss over the training epochs and highlighting the epoch where each model achieved its best validation performance.\n",
        "\n",
        "This is important to track the learning process of each model and evaluate their overall performance, and potentially assess for overfitting to the training data, in which training loss can be low but the validation loss is not decreasing. This could require changing model parameters such as `hidden_size`, the number of hidden layers in the network, or using more regularisation techniques.\n",
        "\n",
        "However, for these simple models, we will see that the models train well and reaching best performance quickly."
      ],
      "metadata": {
        "id": "b9hnbBAlebtW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = {\n",
        "    \"FFNN\": ffnn_history,\n",
        "    \"GRU\": gru_history,\n",
        "    \"LSTM\": lstm_history\n",
        "}\n",
        "\n",
        "plot_training_histories(histories)"
      ],
      "metadata": {
        "id": "uNmKhN_JUSrd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Validate the model"
      ],
      "metadata": {
        "id": "S0ghqYLZfBHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have trained the models, and checked for possible issues around overfitting, we can start using these to make predictions.\n",
        "\n",
        "To do so, we have created a simple function `predict_random_samples` which predicts the time series of `S` and `I` for random samples of beta and gamma combinations from a provided `SIRTimeSeriesDataset`. The function makes predictions for whichever models you pass, and from the predictions creates a data frame with the predictions as well as the equivalent ground-truth from the actual SIR model."
      ],
      "metadata": {
        "id": "bZymZts1fHIS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_random_samples(models, dataset, model_names, num_samples=9, features=['beta', 'gamma', 'I0'], targets=['S', 'I']):\n",
        "    \"\"\"\n",
        "    Predicts the time series of S and I for random samples of beta and gamma combinations,\n",
        "    compares to ground truth, and returns a DataFrame with predictions and ground truth.\n",
        "\n",
        "    Args:\n",
        "        models: List of trained models [FFNN, GRU, LSTM].\n",
        "        dataset: The dataset containing the input features and target values.\n",
        "        model_names: List of model names [\"FFNN\", \"GRU\", \"LSTM\"].\n",
        "        num_samples: Number of random samples to generate (default: 9).\n",
        "        features: List of feature names (default: ['beta', 'gamma']).\n",
        "        targets: List of target names (default: ['S', 'I']).\n",
        "\n",
        "    Returns:\n",
        "        df_long: A Pandas DataFrame containing predictions, ground truth, and metadata.\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "\n",
        "    # Get unique beta and gamma and I0 combinations (ignoring replicates)\n",
        "    unique_combinations = [tuple(param_set[:3]) for param_set in dataset.param_sets]  # Extract only beta, gamma, I0\n",
        "    unique_combinations = list(set(unique_combinations))  # Remove duplicates\n",
        "    selected_combinations = random.sample(unique_combinations, num_samples)\n",
        "    for model, model_name in zip(models, model_names):\n",
        "      model.eval()  # Set model to evaluation mode\n",
        "      for beta, gamma, I0 in selected_combinations:\n",
        "        # Normalize features using the dataset's scaler\n",
        "        features_values = np.array([beta, gamma, I0])  # Create features array\n",
        "        features_normalized = dataset.feature_scaler.transform(features_values.reshape(1, -1)).flatten()\n",
        "\n",
        "        # Normalize features using the dataset's scaler\n",
        "        features_normalized = dataset.feature_scaler.transform(features_values.reshape(1, -1)).flatten()\n",
        "\n",
        "        # Create input tensor\n",
        "        inputs = torch.tensor(features_normalized, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "\n",
        "        # Predict using the model\n",
        "        with torch.no_grad():\n",
        "            predictions = model(inputs).cpu().numpy().squeeze()\n",
        "\n",
        "        # Store predictions and ground truth for each replicate\n",
        "        for t in range(predictions.shape[0]):\n",
        "            all_predictions.append({\n",
        "                'model': model_name,  # Add model name\n",
        "                **dict(zip(features, features_values)),\n",
        "                **dict(zip(targets, predictions[t]*test_dataset.N)),\n",
        "                't': t\n",
        "            })\n",
        "\n",
        "    # Create DataFrame for plotting\n",
        "    df_predictions = pd.DataFrame(all_predictions)\n",
        "    df_predictions = df_predictions.assign(replicate = 0)\n",
        "    df_predictions = df_predictions.assign(N = test_dataset.N)\n",
        "    df_predictions = df_predictions.assign(R = test_dataset.N - df_predictions[\"I\"] - df_predictions[\"S\"])\n",
        "\n",
        "    # Create filtered of test_data\n",
        "    filtered_test_data = dataset.dataframe[test_data[['beta', 'gamma', 'I0']].apply(tuple, axis=1).isin(df_predictions[['beta', 'gamma', 'I0']].apply(tuple, axis=1))]\n",
        "    filtered_test_data = filtered_test_data.assign(model=\"TRUTH\")\n",
        "    # Combine\n",
        "    df_combine = pd.concat([filtered_test_data, df_predictions])\n",
        "\n",
        "    return df_combine\n",
        "\n"
      ],
      "metadata": {
        "id": "YuPFZaIxUXP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we created our datasets, we kept one dataset back to be our test dataset, which will use here by selecting 5 random combinations of $\\beta$, $\\gamma$ and `I0` and creating the predictions of each model as well as the true SIR model runs."
      ],
      "metadata": {
        "id": "DCD9CWCHfvUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_predictions = predict_random_samples(\n",
        "    models = [ffnn_model, gru_model, lstm_model],\n",
        "    dataset = test_dataset,\n",
        "    model_names = [\"FFNN\", \"GRU\", \"LSTM\"],\n",
        "    num_samples = 5\n",
        "    )"
      ],
      "metadata": {
        "id": "2H7pr6ZlOhwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can then wrangle this into long data and return to our beloved ggplot style for plotting the model predictions and the replicates of the stochastic SIR model."
      ],
      "metadata": {
        "id": "n1OVTwXjggI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape dataframe into tidy long-format\n",
        "df_long_preds = df_predictions.melt(\n",
        "    id_vars=[\"t\", \"gamma\", \"beta\", \"I0\",\"replicate\", \"model\"],  # Include model in id_vars\n",
        "    value_vars=[\"S\", \"I\", \"R\"],\n",
        "    var_name=\"Compartment\",\n",
        "    value_name=\"Value\",\n",
        ")\n",
        "\n",
        "# Add unique identifier for group plotting\n",
        "df_long_preds = df_long_preds.assign(\n",
        "    uid=df_long_preds[\"Compartment\"]\n",
        "    + df_long_preds[\"replicate\"].astype(str)\n",
        "    + df_long_preds[\"model\"].astype(str)\n",
        ")\n",
        "\n",
        "# Add alpha value column, 1 if model is not TRUTH, 0.5 otherwise\n",
        "df_long_preds = df_long_preds.assign(\n",
        "    alpha=df_long_preds[\"model\"].apply(lambda x: 0.5 if x == \"TRUTH\" else 1)\n",
        ")\n",
        "\n",
        "df_long_preds[\"alpha\"] = df_long_preds[\"alpha\"].astype(str)\n",
        "\n",
        "# Add facet identifier for group plotting\n",
        "df_long_preds = df_long_preds.assign(\n",
        "    facet=\"Î² = \"\n",
        "    + df_long_preds[\"beta\"].round(2).astype(str)\n",
        "    + \",\\n\"\n",
        "    + \"Î³ = \"\n",
        "    + df_long_preds[\"gamma\"].round(2).astype(str)\n",
        "    + \",\\n\"\n",
        "    + \"I0 = \"\n",
        "    + df_long_preds[\"I0\"].round(2).astype(str)\n",
        ")\n",
        "\n",
        "p = (\n",
        "    ggplot(\n",
        "        df_long_preds,\n",
        "        aes(x=\"t\", y=\"Value\", group=\"uid\", color=\"model\", alpha = \"alpha\"),\n",
        "    )\n",
        "    + geom_line()\n",
        "    + facet_grid(\"Compartment\", \"facet\", scales=\"free\")\n",
        "    + theme_bw()\n",
        ")\n",
        "ggplot.show(p)"
      ],
      "metadata": {
        "id": "QmrDsUQwOm_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessment 2."
      ],
      "metadata": {
        "id": "GC1fx_gqvhtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please head to the [Assessment Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdLzBlKhREQTFVnxE4M6wANYWw98oAAP2JJ0E_5hoWr6pWk0w/viewform?usp=header) and complete Section 2.\n",
        "\n",
        "**Keep your Google Form open in a tab once you have completed section 2**\n",
        "\n",
        "> Hint: To answer the questions you may need to write new cells with code to generate plots to answer some of the questions. To save these plots from Colab, Right Click > Save Image As\n",
        "\n",
        "![Assessment 2](https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/assessment_2.png)"
      ],
      "metadata": {
        "id": "3b5-zw1mYuCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Run the model"
      ],
      "metadata": {
        "id": "s5DxAghBYzj0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The section below will allow you to engage in a hands-on approach to actually using your trained emulator!\n",
        "\n",
        "Everything that we have done so far looks good, however we've not actually shown you how to use the emulator, below we have written a little script that allows you to interface with the emulators you've trained and the SIR model such that you can visually see the outputs of your emulators in real time.\n",
        "\n",
        "Have a go and try different parameter combinations, can you guess what will happen if you feed in a beta and gamma value that sits outside of the trained range?"
      ],
      "metadata": {
        "id": "4UOxk-8dQtvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Interactive Surrogate Testing\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "\n",
        "def run_interactive_comparison(models, dataset, model_names):\n",
        "    \"\"\"\n",
        "    Interactive widget to compare model predictions with actual SIR simulations\n",
        "    for user-defined parameter values.\n",
        "    \"\"\"\n",
        "    # Default values\n",
        "    default_beta = 0.3\n",
        "    default_gamma = 0.1\n",
        "    default_I0 = 3\n",
        "\n",
        "    # Create widgets\n",
        "    beta_slider = widgets.FloatSlider(\n",
        "        value=default_beta,\n",
        "        min=0.05,\n",
        "        max=0.8,\n",
        "        step=0.01,\n",
        "        description='Beta:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        orientation='horizontal',\n",
        "        readout=True,\n",
        "        readout_format='.2f',\n",
        "    )\n",
        "\n",
        "    gamma_slider = widgets.FloatSlider(\n",
        "        value=default_gamma,\n",
        "        min=0.05,\n",
        "        max=0.8,\n",
        "        step=0.01,\n",
        "        description='Gamma:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        orientation='horizontal',\n",
        "        readout=True,\n",
        "        readout_format='.2f',\n",
        "    )\n",
        "\n",
        "    I0_slider = widgets.FloatSlider(\n",
        "        value=default_I0,\n",
        "        min=0,\n",
        "        max=20,\n",
        "        step=1,\n",
        "        description='I0:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        orientation='horizontal',\n",
        "        readout=True,\n",
        "        readout_format='.2f',\n",
        "    )\n",
        "\n",
        "    num_replicates_slider = widgets.IntSlider(\n",
        "        value=10,\n",
        "        min=1,\n",
        "        max=100,\n",
        "        step=1,\n",
        "        description='Replicates:',\n",
        "        disabled=False,\n",
        "        continuous_update=False,\n",
        "        orientation='horizontal',\n",
        "        readout=True,\n",
        "    )\n",
        "\n",
        "    run_button = widgets.Button(\n",
        "        description='Run Simulation',\n",
        "        disabled=False,\n",
        "        button_style='success',\n",
        "        tooltip='Click to run the simulation',\n",
        "        icon='play'\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    def run_simulation(b):\n",
        "        beta = beta_slider.value\n",
        "        gamma = gamma_slider.value\n",
        "        I0 = I0_slider.value\n",
        "        reps = num_replicates_slider.value\n",
        "\n",
        "        with output:\n",
        "            clear_output(wait=True)\n",
        "\n",
        "            # Run actual SIR model with these parameters\n",
        "            print(f\"Running SIR model with beta={beta}, gamma={gamma}, I0={I0}, replicates={reps}\")\n",
        "            sir_results = run_model_with_replicates(beta=beta, gamma=gamma, I0=I0, reps=reps)\n",
        "\n",
        "            # Normalize inputs for surrogate models\n",
        "            features_values = np.array([beta, gamma, I0])\n",
        "            features_normalized = dataset.feature_scaler.transform(features_values.reshape(1, -1)).flatten()\n",
        "            inputs = torch.tensor(features_normalized, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get predictions from all models\n",
        "            model_predictions = {}\n",
        "            for model, model_name in zip(models, model_names):\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    preds = model(inputs).cpu().numpy().squeeze()\n",
        "                    # Convert to actual compartment values (S, I)\n",
        "                    S_pred = preds[:, 0] * dataset.N\n",
        "                    I_pred = preds[:, 1] * dataset.N\n",
        "                    R_pred = dataset.N - S_pred - I_pred\n",
        "                    model_predictions[model_name] = {\n",
        "                        'S': S_pred,\n",
        "                        'I': I_pred,\n",
        "                        'R': R_pred\n",
        "                    }\n",
        "\n",
        "            # Create time points\n",
        "            timepoints = dataset.timepoints\n",
        "\n",
        "            # Plot results\n",
        "            fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "            compartments = ['S', 'I', 'R']\n",
        "            colors = ['blue', 'red', 'green']\n",
        "\n",
        "            for i, comp in enumerate(compartments):\n",
        "                ax = axes[i]\n",
        "\n",
        "                # Plot actual SIR results\n",
        "                for r in range(reps):\n",
        "                    replicate_data = sir_results[sir_results['replicate'] == r]\n",
        "                    ax.plot(replicate_data['t'], replicate_data[comp],\n",
        "                            '-', color=\"grey\", alpha=0.3, markersize=3,\n",
        "                            label=f'SIR {comp}' if r == 0 else None)\n",
        "\n",
        "                # Plot model predictions\n",
        "                for j, (model_name, preds) in enumerate(model_predictions.items()):\n",
        "                    ax.plot(timepoints, preds[comp], '-',\n",
        "                            label=f'{model_name} {comp}',\n",
        "                            linewidth=2, alpha=0.8)\n",
        "\n",
        "                ax.set_title(f'{comp} Compartment')\n",
        "                ax.set_xlabel('Time')\n",
        "                ax.set_ylabel('Population')\n",
        "                ax.legend()\n",
        "                ax.grid(alpha=0.3)\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    run_button.on_click(run_simulation)\n",
        "\n",
        "    # Create layout and display widgets\n",
        "    display(widgets.VBox([\n",
        "        widgets.HBox([beta_slider, gamma_slider, I0_slider]),\n",
        "        widgets.HBox([num_replicates_slider, run_button]),\n",
        "        output\n",
        "    ]))\n",
        "\n",
        "    # Run once with default values\n",
        "    run_button.click()\n",
        "\n",
        "# Run the interactive comparison\n",
        "print(\"Use the sliders to change beta, gamma, I0 values, then click 'Run Simulation'\")\n",
        "\n",
        "run_interactive_comparison(\n",
        "    models=[ffnn_model, gru_model, lstm_model],\n",
        "    dataset=train_dataset,\n",
        "    model_names=[\"FFNN\", \"GRU\", \"LSTM\"]\n",
        ")"
      ],
      "metadata": {
        "id": "QtlMmMSEO20l",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Evaluate the model\n"
      ],
      "metadata": {
        "id": "1qPwgnmUc745"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Error metrics provide essential information about model performance. In epidemiological modeling, understanding how well our models predict disease spread under different transmission (beta) and recovery (gamma) parameters is crucial. The heatmaps we'll generate visualize performance across the parameter space, highlighting where models excel and where they struggle.\n",
        "These standardized metrics (scaled from 0-1) allow for fair comparisons between different models and across different parameter combinations. Higher values (greener colors) always indicate better performance.\n",
        "\n",
        "In essence, the code below systematically evaluates the accuracy of three different neural network models (FFNN, GRU, LSTM) in emulating an SIR epidemiological model. By generating high-resolution heatmaps, it visually compares their performance across a range of infection and recovery rates, providing valuable insights into the models' strengths and weaknesses in predicting disease dynamics, and how the performance varies across the parameter space that was used in training and that which is outside this space.\n",
        "\n",
        "**Don't worry too much about this code, and focus on looking at the outputs and thinking about whether the results surprise you**"
      ],
      "metadata": {
        "id": "RBINi857c-67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Higher Resolution Standardized Error Metrics Heatmap\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "def calculate_metrics(true_values, predicted_values):\n",
        "    \"\"\"Calculate multiple error metrics between true and predicted values.\"\"\"\n",
        "    mse = mean_squared_error(true_values, predicted_values)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(true_values, predicted_values)\n",
        "\n",
        "    # For RÂ², handle potential errors (RÂ² can be negative if predictions are poor)\n",
        "    try:\n",
        "        r2 = r2_score(true_values, predicted_values)\n",
        "        # Clip RÂ² to prevent extreme negative values from skewing the heatmap\n",
        "        r2 = max(r2, -1)\n",
        "    except:\n",
        "        r2 = -1\n",
        "\n",
        "    return {\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'RÂ²': r2,\n",
        "        'MAE': mae\n",
        "    }\n",
        "\n",
        "def plot_high_resolution_heatmaps(models, dataset, model_names, n_replicates=100, grid_resolution=20):\n",
        "    \"\"\"\n",
        "    Generate error heatmaps across beta-gamma space with standardized metrics (0-1 scale)\n",
        "    with one plot per metric and three panels per plot (one for each model).\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    models : list\n",
        "        List of trained models\n",
        "    dataset : Dataset\n",
        "        Dataset containing the input features and target values\n",
        "    model_names : list\n",
        "        List of model names as strings\n",
        "    n_replicates : int\n",
        "        Number of SIR model replicates to run for each parameter combination\n",
        "    grid_resolution : int\n",
        "        Number of grid points in each dimension (higher = smaller squares)\n",
        "    \"\"\"\n",
        "    print(f\"Generating high-resolution standardized error heatmaps with {grid_resolution}x{grid_resolution} grid...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Define grid resolution (increased from previous 12x12)\n",
        "    n_points = grid_resolution\n",
        "\n",
        "    gamma_lower = max(param_ranges[\"gamma\"][0] - 0.5, 0.01)\n",
        "    gamma_upper = param_ranges[\"gamma\"][1] + 0.5\n",
        "    beta_lower = max((param_ranges[\"R0\"][0]*gamma_lower) - 0.5, 0.01)\n",
        "    beta_upper = (param_ranges[\"R0\"][1]*param_ranges[\"gamma\"][1]) + 0.5\n",
        "    I0 = (param_ranges[\"I0\"][0] + param_ranges[\"I0\"][1]) / 2\n",
        "\n",
        "    # Define beta and gamma ranges - extend to full 0-1 range\n",
        "    # Starting slightly above 0 to avoid division by zero when calculating R0\n",
        "    beta_values = np.linspace(beta_lower, beta_upper, n_points)\n",
        "    gamma_values = np.linspace(gamma_lower, gamma_upper, n_points)\n",
        "\n",
        "    # Create meshgrid for visualization\n",
        "    beta_grid, gamma_grid = np.meshgrid(beta_values, gamma_values)\n",
        "\n",
        "    # Define error metrics to track\n",
        "    metrics = ['MSE', 'RMSE', 'RÂ²', 'MAE']\n",
        "\n",
        "    # Initialize error arrays - shape: (models, metrics, gamma_points, beta_points)\n",
        "    error_grids = {\n",
        "        model_name: {\n",
        "            metric: np.zeros_like(beta_grid)\n",
        "            for metric in metrics\n",
        "        } for model_name in model_names\n",
        "    }\n",
        "\n",
        "    # Create a progress counter\n",
        "    total_points = len(beta_values) * len(gamma_values)\n",
        "    completed_points = 0\n",
        "\n",
        "    # Track which points are outside the training distribution\n",
        "    is_outside_training = np.zeros_like(beta_grid, dtype=bool)\n",
        "\n",
        "    # Loop through each point in the grid\n",
        "    for i, gamma in enumerate(gamma_values):\n",
        "        for j, beta in enumerate(beta_values):\n",
        "            # Check if this point is outside the typical training distribution\n",
        "            is_outside_training[i, j] = (gamma > beta)\n",
        "\n",
        "            # Run the actual SIR model with multiple replicates\n",
        "            sir_results = run_model_with_replicates(beta=beta, gamma=gamma, I0 = I0, reps=n_replicates)\n",
        "\n",
        "            # Calculate mean of SIR replicates as ground truth\n",
        "            sir_mean = sir_results.groupby('t').mean()[['S', 'I', 'R']].reset_index()\n",
        "\n",
        "            # Normalize input for surrogate models\n",
        "            features_values = np.array([beta, gamma, I0])\n",
        "            features_normalized = dataset.feature_scaler.transform(features_values.reshape(1, -1)).flatten()\n",
        "            inputs = torch.tensor(features_normalized, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Get predictions from each model\n",
        "            for model, model_name in zip(models, model_names):\n",
        "                model.eval()\n",
        "                with torch.no_grad():\n",
        "                    preds = model(inputs).cpu().numpy().squeeze()\n",
        "\n",
        "                    # Convert to actual compartment values (S, I, R)\n",
        "                    S_pred = preds[:, 0] * dataset.N\n",
        "                    I_pred = preds[:, 1] * dataset.N\n",
        "                    R_pred = dataset.N - S_pred - I_pred\n",
        "\n",
        "                    # Combine all compartments for overall metrics\n",
        "                    all_true = np.concatenate([sir_mean['S'], sir_mean['I'], sir_mean['R']])\n",
        "                    all_pred = np.concatenate([S_pred, I_pred, R_pred])\n",
        "\n",
        "                    # Calculate metrics\n",
        "                    metrics_results = calculate_metrics(all_true, all_pred)\n",
        "\n",
        "                    # Store in the error grids\n",
        "                    for metric_name, metric_value in metrics_results.items():\n",
        "                        error_grids[model_name][metric_name][i, j] = metric_value\n",
        "\n",
        "            # Update progress\n",
        "            completed_points += 1\n",
        "            if completed_points % 20 == 0 or completed_points == total_points:\n",
        "                elapsed = time.time() - start_time\n",
        "                estimated_total = elapsed / completed_points * total_points\n",
        "                remaining = estimated_total - elapsed\n",
        "                percent_complete = 100 * completed_points / total_points\n",
        "                print(f\"Progress: {percent_complete:.1f}% complete ({completed_points}/{total_points} points). \" +\n",
        "                      f\"Time remaining: ~{remaining/60:.1f} minutes\")\n",
        "\n",
        "    print(f\"Heatmap generation completed in {(time.time() - start_time)/60:.2f} minutes\")\n",
        "\n",
        "    # Create a custom colormap (green for good, red for bad)\n",
        "    custom_cmap = LinearSegmentedColormap.from_list('custom_cmap', ['crimson', 'gold', 'darkgreen'])\n",
        "\n",
        "    # Standardize metrics to 0-1 scale\n",
        "    standardized_grids = {\n",
        "        model_name: {\n",
        "            metric: np.zeros_like(error_grids[model_name][metric])\n",
        "            for metric in metrics\n",
        "        } for model_name in model_names\n",
        "    }\n",
        "\n",
        "    # Process each metric\n",
        "    for metric in metrics:\n",
        "        # Collect all values for this metric across all models\n",
        "        all_values = []\n",
        "        for model_name in model_names:\n",
        "            all_values.extend(error_grids[model_name][metric].flatten())\n",
        "\n",
        "        # Get min and max for scaling (with protection against outliers)\n",
        "        if metric == 'RÂ²':\n",
        "            # RÂ² is already in a known range [-1, 1]\n",
        "            # Transform to [0, 1] where 1 is best (highest RÂ²)\n",
        "            for model_name in model_names:\n",
        "                standardized_grids[model_name][metric] = (error_grids[model_name][metric] + 1) / 2\n",
        "        else:\n",
        "            # For error metrics (MSE, RMSE, MAE), lower is better\n",
        "            # Use percentiles to avoid extreme outliers\n",
        "            min_val = np.percentile(all_values, 5)\n",
        "            max_val = np.percentile(all_values, 95)\n",
        "\n",
        "            # Transform to [0, 1] where 1 is best (lowest error)\n",
        "            for model_name in model_names:\n",
        "                normalized = (error_grids[model_name][metric] - min_val) / (max_val - min_val)\n",
        "                # Clip to [0, 1] in case of values outside the percentile range\n",
        "                normalized = np.clip(normalized, 0, 1)\n",
        "                # Invert so that 1 means best performance (lowest error)\n",
        "                standardized_grids[model_name][metric] = 1 - normalized\n",
        "\n",
        "    # Calculate the min/max beta based on R0 and gamma (for plotting boundaries)\n",
        "    min_beta = param_ranges[\"R0\"][0] * param_ranges[\"gamma\"][0]\n",
        "    max_beta = param_ranges[\"R0\"][1] * param_ranges[\"gamma\"][1]\n",
        "\n",
        "    # Create one figure per metric (with three panels for the models)\n",
        "    for metric in metrics:\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 5), constrained_layout=True)\n",
        "\n",
        "        for n, model_name in enumerate(model_names):\n",
        "            ax = axes[n]\n",
        "\n",
        "            # Create the heatmap with standardized values\n",
        "            # Use shading='gouraud' for smoother interpolation between grid cells\n",
        "            im = ax.pcolormesh(beta_grid, gamma_grid, standardized_grids[model_name][metric],\n",
        "                               cmap=custom_cmap,\n",
        "                               vmin=0, vmax=1,\n",
        "                               shading='gouraud')  # Use 'gouraud' for smooth interpolation\n",
        "\n",
        "            # Add color bar\n",
        "            cbar = fig.colorbar(im, ax=ax)\n",
        "            if metric == 'RÂ²':\n",
        "                cbar.set_label(f'Standardized {metric} (higher is better)')\n",
        "            else:\n",
        "                cbar.set_label(f'Standardized {metric} (higher = lower error)')\n",
        "\n",
        "            # Add R0 = 1 line (where beta = gamma)\n",
        "            ax.plot(beta_values, beta_values, 'r--', linewidth=2, label='Râ‚€ = 1')\n",
        "\n",
        "            # Add contour lines for R0 values\n",
        "            for r0 in [2, 4]:\n",
        "                r0_gamma = beta_values / r0\n",
        "                valid_idx = r0_gamma <= np.max(gamma_values)\n",
        "                if any(valid_idx):\n",
        "                    ax.plot(beta_values[valid_idx], r0_gamma[valid_idx],\n",
        "                            'k:', linewidth=1, alpha=0.5, label=f'Râ‚€ = {r0}')\n",
        "\n",
        "            # Add training region boundary (using calculated beta values)\n",
        "            ax.add_patch(plt.Rectangle(\n",
        "                (min_beta, param_ranges[\"gamma\"][0]),\n",
        "                max_beta - min_beta,\n",
        "                param_ranges[\"gamma\"][1] - param_ranges[\"gamma\"][0],\n",
        "                fill=False, edgecolor='blue', linestyle='--', alpha=0.7,\n",
        "                label='Training Region'))\n",
        "\n",
        "            # Set labels and title\n",
        "            ax.set_xlabel('Beta (infection rate)')\n",
        "            ax.set_ylabel('Gamma (recovery rate)')\n",
        "            ax.set_title(f'{model_name}')\n",
        "\n",
        "            # Set limits for our beta and gamma ranges\n",
        "            ax.set_xlim(beta_lower, beta_upper)\n",
        "            ax.set_ylim(gamma_lower, gamma_upper)\n",
        "\n",
        "            # Add grid\n",
        "            ax.grid(alpha=0.2, linestyle=':')\n",
        "\n",
        "            # Add legend\n",
        "            ax.legend(loc='upper right', fontsize=8)\n",
        "\n",
        "            # For RÂ², show the original RÂ² = 0 contour\n",
        "            if metric == 'RÂ²':\n",
        "                # RÂ² = 0 corresponds to standardized value of 0.5\n",
        "                ax.contour(beta_grid, gamma_grid, standardized_grids[model_name][metric],\n",
        "                          levels=[0.5], colors=['k'], linewidths=1.5, alpha=0.7,\n",
        "                          linestyles='dashed')\n",
        "\n",
        "        # Main title for the figure\n",
        "        if metric == 'RÂ²':\n",
        "            metric_description = \"(higher values = better fit)\"\n",
        "        else:\n",
        "            metric_description = \"(higher values = lower error)\"\n",
        "\n",
        "        fig.suptitle(f'Model Comparison: Standardized {metric} across Parameter Space {metric_description}',\n",
        "                    fontsize=16)\n",
        "\n",
        "        # Save figure\n",
        "        plt.savefig(os.path.join(output_dir, f\"high_res_error_{metric}.png\"), dpi=150)\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "GZRicgKKP3tS",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the high-resolution analysis\n",
        "print(\"This analysis visualizes model performance across the parameter space with higher grid resolution.\")\n",
        "print(\"For all metrics, higher values (greener) indicate better performance.\")\n",
        "\n",
        "# Prompt user for desired resolution\n",
        "resolution = 20  # Default resolution\n",
        "print(f\"Using {resolution}x{resolution} grid (higher = smaller squares, but longer runtime)\")\n",
        "\n",
        "# Plot heat maps to show performance of our surrogates over a parameter range\n",
        "plot_high_resolution_heatmaps(\n",
        "    models=[ffnn_model, gru_model, lstm_model],\n",
        "    dataset=train_dataset,\n",
        "    model_names=[\"FFNN\", \"GRU\", \"LSTM\"],\n",
        "    n_replicates=10,\n",
        "    grid_resolution=resolution\n",
        ")\n"
      ],
      "metadata": {
        "id": "Hz3_lVj7t6Gu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When looking at the heatmap above, think through these 3 questions to help guide your thinking and answers for the final assessment:\n",
        "\n",
        "> **1. Do the patterns of model performance across the beta-gamma parameter space make intuitive sense given your understanding of SIR dynamics?** For example, are there regions where all models perform well or poorly? If so, how do those regions relate to the basic reproductive number (R0), which is influenced by beta and gamma? Can you explain why certain parameter combinations might be more challenging for the models to predict accurately?\n",
        "\n",
        "> **2. What specific features or boundaries can you identify in the heatmaps, and how might they be linked to the underlying SIR model or the training data used for the neural networks?** Do you notice any sharp transitions in performance, or regions where one model significantly outperforms others? How might these features relate to the training region boundary, the line representing R0 = 1, or the contour lines for different R0 values? Is the training region shown with the blue box correct? Is this the entire region that it was trained on, and if not not does this explain the heatmap patterns?\n",
        "\n",
        "> **3. Considering the overall performance visualized in the heatmaps, which model appears to be the most robust and reliable surrogate for the SIR model across a wide range of parameter values?** What specific evidence from the heatmaps supports your conclusion? Are there scenarios or parameter ranges where a particular model might be preferred over others, and why? How might you use these insights to make informed decisions about model selection for future epidemiological studies or simulations?"
      ],
      "metadata": {
        "id": "MD7h2WVVlmwe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessment 3."
      ],
      "metadata": {
        "id": "2qNtdLUPvm1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please head to the [Assessment Google Form](https://docs.google.com/forms/d/e/1FAIpQLSdLzBlKhREQTFVnxE4M6wANYWw98oAAP2JJ0E_5hoWr6pWk0w/viewform?usp=header) and complete Section 3.\n",
        "\n",
        "**Keep your Google Form open in a tab once you have completed section 3**\n",
        "\n",
        "> Hint: To answer the questions you may need to write new cells with code to generate plots to answer some of the questions. To save these plots from Colab, Right Click > Save Image As\n",
        "\n",
        "![Assessment 3](https://raw.githubusercontent.com/OJWatson/emidm/refs/heads/main/examples/imgs/assessment_3.png)"
      ],
      "metadata": {
        "id": "VN930vq_vqBB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion"
      ],
      "metadata": {
        "id": "-yArxdWatsum"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, we explored the process of building and training deep learning surrogates for infectious disease modeling using the SIR model as an example. We covered the following key steps:\n",
        "\n",
        "1. **Data Generation:** We used the `emidm` package to simulate SIR model dynamics with varying parameters using Latin Hypercube Sampling (LHS) to systematically explore the parameter space and generate training, validation, and test datasets.\n",
        "\n",
        "2. **Data Preparation:** We created a custom PyTorch `Dataset` class (`SIRTimeSeriesDataset`) to efficiently handle the time series data and implemented `DataLoader` for batch processing during training.\n",
        "\n",
        "3. **Model Building and Training:** We defined and trained three different neural network architectures:\n",
        "   - **Feedforward Neural Network (FFNN):** A basic neural network with multiple layers.\n",
        "   - **Gated Recurrent Unit (GRU):** A recurrent neural network designed for sequential data.\n",
        "   - **Long Short-Term Memory (LSTM):** Another recurrent neural network with more complex gating mechanisms.\n",
        "   We trained these models using the Adam optimizer and a learning rate scheduler, monitoring their performance on validation data to prevent overfitting.\n",
        "   - We also applied a sigmoid activation function to the final layer of each model to ensure the predictions were bound between 0 and 1 and included normalization.\n",
        "\n",
        "4. **Model Validation:** We assessed the trained models' performance by predicting on a held-out test dataset and comparing the predictions to the ground truth SIR model simulations.\n",
        "\n",
        "5. **Visualization:** We visualized the training history, showing the training and validation losses over epochs, and highlighted the best-performing epochs for each model."
      ],
      "metadata": {
        "id": "oZgsO7AFg_mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extensions for Further Exploration"
      ],
      "metadata": {
        "id": "mVDiLOqwhlUG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you've completed the main sections of this notebook and have some extra time, consider exploring these extensions to deepen your understanding of surrogate modeling."
      ],
      "metadata": {
        "id": "yBG5Z7SyuzHL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Hyperparameter Tuning\n",
        "\n",
        "**Challenge:** Experiment with different hyperparameters of the neural network models (e.g., hidden size, number of layers, dropout rate, learning rate) to see how they affect performance.\n",
        "\n",
        "**Hint:** Use a grid search or random search approach to systematically explore different hyperparameter combinations and evaluate their impact on validation loss.\n",
        "\n",
        "**Code Snippet (Grid Search Example):**\n",
        "\n",
        "```python\n",
        "import itertools\n",
        "\n",
        "# Define hyperparameter ranges\n",
        "hidden_sizes = [32, 64, 128]\n",
        "num_layers = [1, 2, 3]\n",
        "dropout_rates = [0.1, 0.2, 0.3]\n",
        "\n",
        "# Create all possible combinations\n",
        "hyperparameter_combinations = list(itertools.product(hidden_sizes, num_layers, dropout_rates))\n",
        "\n",
        "# Loop through combinations and train models\n",
        "for hidden_size, num_layers, dropout_rate in hyperparameter_combinations:\n",
        "\n",
        "# Create and train model with current hyperparameters\n",
        "# ...\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "XmiDGGtShu0t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Capturing Stochastic Uncertainty\n",
        "\n",
        "**Challenge:** Modify the models to predict the variance in stochastic replicates over time, rather than just the point estimate. This can be used to create a surrogate that captures stochastic uncertainty, providing a more comprehensive representation of the SIR model's behavior.\n",
        "\n",
        "**Hint:** Instead of predicting only the mean values of S and I, train the models to predict both the mean and variance (or standard deviation) for each time step. You'll need to adjust the output layer of the models and modify the loss function to account for both mean and variance predictions. You can consider using metrics like the negative log-likelihood (NLL) for evaluating the performance of models predicting distributions.\n",
        "\n",
        "**Code Snippet (Modifying the FFNN model):**\n",
        "```python\n",
        "class FFNN(nn.Module):\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Output now includes both mean and variance\n",
        "        output = self.network(x)\n",
        "        output = output.view(-1, self.time_steps, self.output_channels, 2)\n",
        "        mean = self.sigmoid(output[:, :, :, 0])\n",
        "        variance = self.softplus(output[:, :, :, 1])  \n",
        "        return mean, variance\n",
        "```\n",
        "\n",
        "> Further Hint: Instead of predicting only the mean values of S and I, train the models to predict both the mean and variance (or standard deviation) for each time step. You'll need to adjust the output layer of the models and modify the loss function to account for both mean and variance predictions. **Importantly, you'll also need to change the SIRTimeSeriesDataset class to ensure that all stochastic replicates for a given parameter set are available during training, as the model now needs to learn the distribution of outcomes.** You can consider using metrics like the negative log-likelihood (NLL) for evaluating the performance of models predicting distributions."
      ],
      "metadata": {
        "id": "9dDnfJWWiRCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Surrogate-Assisted Inference\n",
        "\n",
        "**Challenge:** Use the trained surrogate model to perform tasks like parameter estimation or sensitivity analysis, which are typically computationally expensive with the original SIR model.\n",
        "\n",
        "**Hint:** You can use optimization algorithms to find parameter values that minimize the difference between the surrogate's predictions and observed data. For parameter estimation, you can define an objective function that calculates the loss between the surrogate's predictions and the observed data for a given set of parameters. Then, use an optimization algorithm like `scipy.optimize.minimize` to find the parameter values that minimize this loss. For sensitivity analysis, you can vary the input parameters of the surrogate model and observe the corresponding changes in the output to understand the model's sensitivity to different parameters.\n",
        "\n",
        "**Code Snippet (Parameter Estimation):**\n",
        "\n",
        "```python\n",
        "scipy.optimize import minimize\n",
        "\n",
        "def objective_function(params, surrogate_model, observed_data):\n",
        "  # ... (calculate surrogate predictions using params) ...\n",
        "  # ... (calculate difference between predictions and observed_data) ...\n",
        "  return loss\n",
        "\n",
        "# Perform optimization\n",
        "result = minimize(objective_function, initial_params, args=(surrogate_model, observed_data))\n",
        "```\n",
        "\n",
        "> > Further Hint: You can generate your own observed data by simulating an SIR epidemic using the functions from `emidm`. Or you could get data from a real outbreak from [https://www.reconverse.org/outbreaks/](https://www.reconverse.org/outbreaks/), e.g. the outbreak of influenza A (H1N1) in 1978 at a British boarding school of 763 children and changing your data generation to use a different `N`."
      ],
      "metadata": {
        "id": "PXMu6g1Oj7Jq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedback\n",
        "\n",
        "I would be really grateful for your time in completing the feedback form for this session.\n",
        "\n",
        "You can find the form [here](https://forms.office.com/Pages/ResponsePage.aspx?id=B3WJK4zudUWDC0-CZ8PTBx_JkR_maLNHu6WeXZ96mQhUOEhZNVlVN1dPQkpXVzgwRzVRWDVEMllOQy4u).\n",
        "\n",
        "Thank you and hope you enjoyed the tutorial!"
      ],
      "metadata": {
        "id": "se6ZRzVG7rSe"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}