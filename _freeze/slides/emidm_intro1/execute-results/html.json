{
  "hash": "f0a90bc843d1405eaddecbe1f71ff912",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Introduction to Deep Learning Frameworks with Python and PyTorch\"\nsubtitle: \"DIDE Training Series\"\ndate: \"Thursday, 3 April 2025\"\ndate-format: long\nformat:\n  revealjs:\n    incremental: false\n    theme: [default, custom.scss]\n    footer: \"Introduction to Deep Learning Frameworks with Python and PyTorch ¬∑ DIDE Training Series ¬∑ 03 April 2025\"\n    logo: images/logos.png\n---\n\n\n\n\n# Introduction {.smaller}\n\n- We‚Äôll explore Python basics for R users, a new IDE (Positron), and deep learning concepts.\n- **Goal:** Show you how to use **PyTorch** for training a surrogate infectious disease model.\n\n- Structure:\n  - 2:00 - 2:30: Introduction Slides (python basics, positron, notebooks, deep learning)\n  - 2:30 - 3:00: Sharing Surrogate Notebook (making sure everyone can run Colab, everyone is zen üßò)\n  - 3:00 - 3:30: Hands-on practice running PyTorch and checking it out\n  - 3:30 - 4:00: Extension exercises and wrap up\n\n# 1. Introduction to Python for R Users {.smaller}\n\n## Python vs R: Syntax & Structure Reminders {.smaller}\n\n- **Syntax:** Python uses indentation instead of `{}`. No `<-` assignment (use `=`).\n- **Indexing:** Python indices start at 0 (R starts at 1). Slicing in Python excludes end index.\n- **Data structures:** Python has lists, dicts, tuples (vs R‚Äôs vectors, lists, data frames). Python lists are heterogeneous like R lists.\n- **Example:** \n  - In Python, `mylist = [10, 20, 30]; mylist[0] = 10`. \n  - In R, `myvec <- c(10,20,30); myvec[1] = 10`.\n\n## Python vs R: Deeper Overview {.smaller}\n\n- **Python Workshop:** [Jesse and Paul python-workshop](https://github.com/plietar/python-workshop/tree/main)\n\nFeature               R              Python\n--------------------- -------------- ---------------\nwhitespace            ignored        meaningful\ndata frames & stats   out-of-box     need package\npackages              fussy          easy\noperate on language   yes            no\nmodifying variables   copy-on-modify modify-in-place\nvariable assignment   `<-` (madness) `=` (sane)\n\n- **OJ Reflections on Above Table:** ü§î\n\n## Python vs R: Deeper Overview {.smaller}\n\n- **Python Workshop:** [Jesse and Paul python-workshop](https://github.com/plietar/python-workshop/tree/main)\n- **OJ Edits on Table:** ü§î\n\nFeature               R              Python\n--------------------- -------------- ---------------\nplotting              heavenly       hell\ndocumentation         `roxygen`‚ù§Ô∏è     `docstrings`üòí\ndependencies hell üòà  moderate       frequent\nvirtual environments  less often     standard\n\n# 2. Positron IDE  {.smaller}\n\n## What is Positron? {.smaller}\n\n- **Positron** is a next-generation IDE from Posit (RStudio) built on VS Code, tailored for data science. It feels like a blend of RStudio and VSCode in one interface.\n- **Polyglot support:** Comes configured for both R *and* Python out of the box ‚Äì you can run R scripts and Jupyter Python notebooks in the same environment.\n- **Familiar interface:** Shares RStudio‚Äôs layout (source, console, plots, environment) with VSCode‚Äôs extensibility. This makes the transition easier for R users.\n- **Why use it?** Simplifies working on projects that use R **and** Python together, without switching IDEs. Familiiar IDE experience for R users learning/transitioning to Python.\n\n ([Positron](https://positron.posit.co/)) *Positron IDE combines a Visual Studio Code foundation with an RStudio-like layout, supporting multi-language notebooks and scripts (R and Python) seamlessly.*\n\n## Benefits for R Users {.smaller}\n\n- **Unified Workflow:** Work with `.R` and `.py` files in one place, share workspace and variables. For example, run an R analysis then a Python machine learning step in one project.\n- **Jupyter integration:** Built-in support for Jupyter notebooks (no separate JupyterLab needed).\n- **Extensions:** Leverage VSCode extensions (linting, Git integration, etc.) in Positron. Most VSCode extensions (except a few Microsoft-specific) are available ([Fun with Positron | Andrew Heiss ‚Äì Andrew Heiss](https://www.andrewheiss.com/blog/2024/07/08/fun-with-positron/#:~:text=Extension%20page%20for%20Stan)).\n- **Transition ease:** Minimal setup ‚Äì Positron auto-detects R and Python installations. Familiar shortcuts (e.g. Ctrl+Enter to run code) work for both languages.\n- **REPL-like development experience:** Positron has a REPL-style development environment, similar to RStudio. You can run code, see results, and modify the environment as you go, putting you closer to understanding the code you write. \n- **Bottom line: Positron lowers the barrier for R programmers to start incorporating Python into their workflow within a single IDE.**\n\n## Google Colab\n\n[Intro to Colab Notebook](https://ojwatson.github.io/emidm/examples/notebooks_intro.html){preview-link=\"true\" style=\"text-align: center\"}\n\n# 3. Deep Learning Basics {.smaller}\n\n## What is Deep Learning? {.smaller}\n\n- **Definition:** Deep learning is a subset of machine learning that uses multi-layered neural networks to simulate complex decision-making, akin to the human brain üß†. \n- **Key idea:** Instead of manual feature engineering, a deep neural network learns representations through layers of neurons.\n- **Why it matters:** Achieved breakthroughs in many fields, takes advantage of GPUs, there is a small window where people think AI da üí£ and it's on lots of grant calls... (Truth üí£).\n- **In short:** Deep learning can automatically extract complex patterns from data, making it extremely powerful for modeling nonlinear relationships.\n\n## Key Concepts: Neural Networks {.smaller}\n\n- **Neurons & Layers:** Basic unit is a *neuron* (takes inputs, applies weights and activation). Neurons are organized into layers (input layer -> hidden layers -> output layer). Multiple hidden layers = a ‚Äúdeep‚Äù network.\n- **Activation Functions:** Non-linear functions applied at neurons (e.g. ReLU, sigmoid). They enable networks to learn complex non-linear mappings.\n- **Loss Function:** Metric that quantifies error between the network‚Äôs prediction and true values (e.g. mean squared error, cross-entropy). The network‚Äôs goal is to minimize this during training.\n- **Forward Propagation:** Data flows through the network layer by layer to produce an output (prediction).\n- **Backpropagation:** The training algorithm ‚Äì compute the loss, then propagate the error gradients backward adjusting weights (via gradient descent). This is how the network ‚Äúlearns‚Äù from mistakes.\n- **Optimization:** An optimizer (SGD, Adam, etc.) uses those gradients to update weights iteratively, gradually improving the model‚Äôs performance.\n- **In short:**: At it's core it's quite simple stats, and more complex is the types of neural network architectures for different problems. \n\n## Training Process Recap {.smaller}\n\n- **Initialize** weights (often random small values).\n- **Forward pass:** Input data -> compute outputs and loss.\n- **Backward pass:** Compute gradients of loss w.rt each weight (this is automatic via backpropagation).\n- **Weight update:** Adjust weights using gradients (optimizer step).\n- **Epoch:** A complete pass through the entire training dataset. Training typically requires multiple epochs to converge.\n- **Iterate:** Repeat for many epochs (passes through data) until loss converges or performance is sufficient.\n- **Batch size:** The number of training examples in a mini-batch. Affects training speed, memory usage, and generalization.\n- **Result:** A trained neural network model that hopefully generalizes to new data.\n- Throughout training, we monitor metrics on a **validation set** to avoid overfitting. If validation performance plateaus or worsens, we consider techniques like early stopping or regularization.\n\n# 4. Introduction to PyTorch {.smaller}\n\n## What is PyTorch? {.smaller}\n\n- **PyTorch** is an open-source deep learning framework based on Python (and Torch library). It‚Äôs one of the most popular platforms for deep learning research and deployment.\n- Developed by researchers at **Facebook (Meta)** in 2016, it accelerated from prototyping to production with a flexible, pythonic approach and is arguably one of the most robust and mature software ecosystems for deep learning.\n- **Dynamic computation graph:** PyTorch uses a ‚Äúdefine-by-run‚Äù approach ‚Äì the neural network graph is built on the fly as you run code, making it intuitive to debug and modify.\n- **Why PyTorch?** \n  - *Easy to learn:* Pythonic API feels natural if you know Python.\n  - *Flexibility:* Define complex models imperatively (no static graphs).\n  - *Community & Ecosystem:* Huge community, lots of pre-trained models and tutorials. Backed by Meta and the open-source community (now part of Linux Foundation).\n- **Usage:** Widely used in academia and industry (Meta uses PyTorch to power all its AI workloads) and it has become a go-to framework for many application areas. \n\n## Core Components of PyTorch {.smaller}\n\n- **Tensor:** Fundamental data structure in PyTorch. A tensor is a homogeneous multi-dimensional array (similar to a NumPy array) that can be processed on CPU or GPU. \n  - Example: `torch.tensor([[1,2],[3,4]])` is a 2x2 tensor of integers.\n- **Autograd:** PyTorch‚Äôs automatic differentiation engine. It records operations on tensors so that gradients can be computed via backpropagation. This means if you have a loss computed from tensors, you can call `.backward()` to get gradients of all parameters ‚Äì no manual calculus needed.\n- **`nn` Module:** High-level neural network APIs. `torch.nn` provides building blocks like layers (Linear, Conv2d, LSTM, etc.), activation functions, loss functions, etc., to easily build complex networks.\n  - You define a neural network as a class inheriting `nn.Module`, compose layers in the constructor, and define the forward pass. PyTorch takes care of gradient computation for the parameters.\n- **Optimizers:** `torch.optim` has algorithms like SGD, Adam for updating model parameters based on computed gradients.\n- **DataLoader:** Utility to load and batch your dataset conveniently, with shuffling and parallel loading.\n- **Device:** Specification of where tensors and models should be stored/executed (CPU or GPU). Example: `device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")`\n\n## PyTorch in Action: Basic Operations {.smaller}\n\nLet‚Äôs see a simple example creating a tensor, doing a computation, and using autograd to get gradients:\n\n::: {#a04af992 .cell execution_count=1}\n``` {.python .cell-code}\nimport torch\n\n# Create a tensor with gradient tracking\nx = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n# Perform operations on the tensor\ny = x * 2  # y = [2.0, 4.0, 6.0]\ny_sum = y.sum()  # y_sum = 12.0 (scalar tensor)\n# Compute gradients (dy_sum/dx)\ny_sum.backward()  # backpropagate through the computation\nprint(x.grad)  # Gradient of y_sum w.r.t x\n# Expected output: tensor([2., 2., 2.])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\ntensor([2., 2., 2.])\n```\n:::\n:::\n\n\n- In this example, `y = 2*x`, and `y_sum = 2*x1 + 2*x2 + 2*x3`. The gradient ‚àÇ(y_sum)/‚àÇx = `[2, 2, 2]`, which PyTorch computes for us.\n- This illustrates how **autograd** frees us from manual gradient derivation. We can then use these gradients in an optimizer to update `x` (or, typically, model parameters) during training.\n- PyTorch‚Äôs imperative style means we used standard Python control flow to compute `y` and `y_sum`. The framework built the computation graph dynamically and handled differentiation under the hood.\n\n# 5. Deep Learning in Infectious Disease Modeling {.smaller}\n\n## Why AI in Epidemiology? {.smaller}\n\n- **Data-driven insights:** Infectious disease modeling traditionally relies on differential equations and statistical models. Machine learning (ML) offers an alternative approach to identify patterns in epidemiological data (e.g. forecasting outbreaks from trends).\n- **Deep learning advantages:** Ability to model complex, non-linear relationships. For instance, neural networks can capture patterns in time-series infection data that might be hard to specify in a parametric model.\n- **Large Development Community:** PyTorch has a large and active community, with many pre-trained models and tutorials. Other popular tools such as Jax/flax underpin the development by large tech companies like Google and Meta, which likely predicates a more robust ecosystem and rapid development and contributions from the community.\n\n## Why AI in Epidemiology? {.smaller}\n\n- **Enhancing traditional models:** AI can be combined with mechanistic models. For example, researchers have integrated neural networks with compartmental models to help estimate parameters that are hard to infer otherwise ([\n            Epi-DNNs: Epidemiological priors informed deep neural networks for modeling COVID-19 dynamics - PMC\n        ](https://pmc.ncbi.nlm.nih.gov/articles/PMC9970927/#:~:text=Differential%20equations,to%20solve%20the%20ordinary%20differential)). This can merge domain knowledge (e.g. SIR model structure) with data-driven flexibility.\n- **Physics-informed NN:** There is a trend of *physics-informed neural networks* for epidemic forecasting ([Physics-informed deep learning for infectious disease forecasting](https://pubmed.ncbi.nlm.nih.gov/39876937/#:~:text=Physics,area%20of%20scientific%20machine%20learning)) ‚Äì these models incorporate the known dynamics (e.g. equations of disease spread) into the training of a neural network, ensuring predictions that respect known laws while still learning from data.\n\n![](images/pinn.png){style=\"height:300px;\"}\n\n\n## Why AI in Epidemiology? {.smaller}\n\n- **Surrogate models:** Using AI to approximate the input-output behavior of complex simulation models. This is particularly useful to speed up scenario analysis in infectious disease simulations that would otherwise be computationally intensive.\n- **It is likely going to be everywhere:** [Artificial intelligence for modelling infectious disease epidemics](https://www.nature.com/articles/s41586-024-08564-w)\n\n![](images/ID_ML.png){style=\"height:400px;\"}\n\n## Surrogate Modeling: Bridging Simulation and AI {.smaller}\n\n- **What is a surrogate model?** It‚Äôs an AI model trained to emulate another process or simulation. For infectious diseases, a surrogate model can learn to reproduce the outcomes of a complex epidemic simulation (e.g. an agent-based model or a stochastic SIR model), given certain inputs, but much faster.\n- **Why use it?** Simulations (especially stochastic ones) can be slow when exploring many scenarios or doing real-time forecasting. A surrogate (once trained) runs almost instantly, allowing rapid experimentation or even real-time applications (like outbreak forecast updates).\n- **In epidemiology:** Surrogates can learn the mapping from disease parameters (like transmission rate, recovery rate) to outputs of interest (like peak infected, time series of infections). Then we can query the surrogate for any parameter combination instead of rerunning the simulation.\n- **Considerations:** Surrogate models need a comprehensive training dataset from the simulation (covering the range of scenarios). They should be validated to ensure they accurately reflect the simulation‚Äôs behavior, especially in the regime of interest (e.g. epidemic growth vs decline phases).\n\n# 6. Surrogate Modeling for Epidemic Simulations {.smaller}\n\n## Case Study: Surrogating a Stochastic SIR Model {.smaller}\n\n- We‚Äôll use the classic **SIR model** (Susceptible-Infectious-Recovered) as an example of building a surrogate. In a stochastic SIR simulation, each run with given parameters produces a trajectory of S, I, R over time.\n\n::: {#f3131e10 .cell execution_count=2}\n``` {.python .cell-code}\nfrom emidm.sir import run_sir, run_model_with_replicates, plot_model_outputs\nplot_model_outputs(run_model_with_replicates(run_sir, reps=10))\n```\n\n::: {.cell-output .cell-output-display}\n![](emidm_intro1_files/figure-revealjs/cell-3-output-1.png){width=960 height=480}\n:::\n\n::: {.cell-output .cell-output-display}\n![](emidm_intro1_files/figure-revealjs/cell-3-output-2.png){width=960 height=480}\n:::\n:::\n\n\n## Building the Surrogate {.smaller}\n\n- **1. Generate simulation data:** Sample a range of parameter sets (e.g. transmission rate Œ≤, recovery rate Œ≥, possibly initial population sizes). For each set, run the SIR simulation and record the resulting infection curves (S(t), I(t), R(t) over time).\n- **2. Prepare training dataset:** From these runs, construct input-output pairs for learning. One approach is to provide the parameters (Œ≤, Œ≥, time t) as inputs and the corresponding S, I, R at that time as outputs. (Alternatively, train a model that directly outputs the entire time-series given Œ≤ and Œ≥.)\n- **3. Train a neural network:** Use PyTorch to define a network (for example, a simple feed-forward network or an RNN) that takes in parameters (and time) and outputs the state. Train it on the simulation data by minimizing the error between the network‚Äôs predicted epidemic curve and the simulation‚Äôs actual values.\n- **4. Validate the model:** Test the surrogate on simulation runs it hasn‚Äôt seen. Check that it can predict the trajectory for new Œ≤, Œ≥ with reasonable accuracy (e.g., the peak and timing of infections match the simulation).\n- **5. Use the surrogate:** Now you have a fast approximation of the SIR model. You can sweep through parameter space quickly, perform uncertainty quantification (by sampling many Œ≤, Œ≥ from distributions and getting outcomes rapidly), or embed this surrogate inside larger frameworks (e.g., optimization or real-time forecasting systems).\n\n## Surrogate Notebook {.smaller}\n\n[Training Surrogates](https://github.com/OJWatson/emidm/blob/main/examples/surrogate_notebook.ipynb){preview-link=\"false\"}\n\n# 7. Conclusion and Next Steps {.smaller}\n\n- We introduced Python again for R users but perhaps in a way focussed on actually using IDEs and emulating Rstudio experience.\n- We covered deep learning fundamentals ‚Äì from what neural networks are to how we train them ‚Äì setting the stage for using these techniques in epidemiology.\n- With **PyTorch**, you have a powerful tool to implement deep learning models. We saw how tensors and autograd work, and ran through a simple example.\n- Machine learning and deep learning are increasingly important in infectious disease modeling and surrogate models can accelerate simulation-based research.\n- **Next steps:** \n  - Hands-on practice: try running a simple PyTorch model (e.g., fit a curve) in Positron.\n  - Experiment with the SIR surrogate idea: simulate data in R, train a PyTorch model in Python.\n  - Explore real datasets: Can you use a neural network to forecast cases? How would you incorporate domain knowledge?\n  - Alternative surrogate approaches\n\n# 8. Alernative Surrogate Approaches\n\n## Conditional Variational Autoencoders (cVAEs). What is a cVAE? {.smaller}\n\n- A **Conditional Variational Autoencoder (cVAE)** is an extension of the Variational Autoencoder that incorporates additional information (conditions) into the generation process.\n- **Recap VAE:** A VAE consists of an encoder that compresses input data into a latent distribution (usually a Gaussian in a low-dimensional space) and a decoder that reconstructs data from a sample of that latent distribution. It‚Äôs trained to reconstruct inputs while keeping latent variables meaningful (via a KL divergence regularization).\n- **Conditional part:** In a cVAE, we ‚Äúcondition‚Äù both encoder and decoder on some extra context. For example, when dealing with images of digits, we can provide the digit label as a condition. The encoder then learns latent variables *given* that label, and the decoder can generate data conditioned on a specific label.\n\n## Conditional Variational Autoencoders (cVAEs). What is a cVAE? {.smaller}\n\n- **Effect:** This allows directed generation ‚Äì e.g., ‚Äúgenerate samples that look like digit 5‚Äù or, in our context, ‚Äúgenerate epidemic curves under a certain scenario‚Äù. The latent space then captures variations *other than* the condition. \n  - For digits, the label fixes which digit, and the latent variables can capture style (thickness of writing, slant, etc.).\n  - For epidemics, one could imagine conditioning on, say, the basic reproduction number $R_0$ or other known factors, and the latent variables capture random effects or unknown influences.\n- **Uncertainty modeling:** cVAEs are useful for representing uncertainty. Instead of a single prediction, a cVAE can generate a *distribution* of possible outcomes for a given condition. For instance, given an initial infection count and $R_0$, a cVAE could generate many possible epidemic trajectories (reflecting stochastic variations).\n\n## Potential Application of cVAE in Epidemiology {.smaller}\n\n- **Scenario:** We have many observed epidemic curves (from simulations or real data) under various conditions (e.g. different intervention strategies). We want to model the distribution of epidemic outcomes for a new condition.\n- **Using cVAE:** We could train a cVAE where:\n  - The input to the encoder is an epidemic trajectory with its condition (e.g. ‚Äúwith lockdown‚Äù vs ‚Äúno lockdown‚Äù).\n  - The condition is also fed into the decoder. \n  - The cVAE learns a latent space of other factors (like unobserved socio-behavioral factors or stochastic effects).\n- Once trained, for a given condition (say ‚Äúlockdown from day 30‚Äù), we can sample different latent vectors to generate a variety of plausible epidemic curves that all reflect that condition. This gives a principled way to explore uncertainty and variability in outcomes.\n- **Why cVAE?** Unlike a standard deterministic model, a cVAE provides a distribution of outcomes, not just one. This is valuable for risk assessment ‚Äì e.g., what‚Äôs the worst-case scenario vs best-case scenario under a specific intervention?\n- **Note:** This is an advanced technique and an active research area. It combines deep learning‚Äôs generative power with the need in epidemiology to quantify uncertainty.\n\n",
    "supporting": [
      "emidm_intro1_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}