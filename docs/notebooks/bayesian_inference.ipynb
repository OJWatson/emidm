{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference with Differentiable Models\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/OJWatson/emidm/blob/main/docs/notebooks/bayesian_inference.ipynb)\n",
    "\n",
    "This tutorial demonstrates how to perform Bayesian inference on epidemiological model parameters using **emidm**'s differentiable models and BlackJAX for Hamiltonian Monte Carlo (HMC) sampling.\n",
    "\n",
    "## Why Bayesian Inference?\n",
    "\n",
    "While gradient-based optimization (as shown in the calibration tutorial) gives us point estimates, Bayesian inference provides:\n",
    "\n",
    "1. **Uncertainty quantification**: Full posterior distributions over parameters\n",
    "2. **Prior incorporation**: Include domain knowledge about plausible parameter values\n",
    "3. **Model comparison**: Compare models via marginal likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This tutorial requires JAX and BlackJAX:\n",
    "\n",
    "```bash\n",
    "pip install emidm[jax] blackjax\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from emidm.diff import DiffConfig, run_diff_sir\n",
    "from emidm.optim import mse_loss\n",
    "\n",
    "# Check if blackjax is available\n",
    "try:\n",
    "    import blackjax\n",
    "    HAS_BLACKJAX = True\n",
    "except ImportError:\n",
    "    HAS_BLACKJAX = False\n",
    "    print(\"BlackJAX not installed. Install with: pip install blackjax\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Synthetic Data\n",
    "\n",
    "We'll create synthetic epidemic data with known parameters, then try to recover them with uncertainty estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True parameters\n",
    "BETA_TRUE = 0.35\n",
    "GAMMA = 0.1\n",
    "N_AGENTS = 200\n",
    "T = 40\n",
    "I0 = 5\n",
    "\n",
    "# Generate \"observed\" data\n",
    "key = jax.random.PRNGKey(42)\n",
    "observed = run_diff_sir(\n",
    "    N_agents=N_AGENTS,\n",
    "    I0=I0,\n",
    "    beta=BETA_TRUE,\n",
    "    gamma=GAMMA,\n",
    "    T=T,\n",
    "    config=DiffConfig(tau=0.5, hard=True),\n",
    "    key=key,\n",
    ")\n",
    "\n",
    "observed_I = observed[\"I\"]\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.plot(observed[\"t\"], observed[\"S\"], label=\"S\", color=\"blue\")\n",
    "ax.plot(observed[\"t\"], observed[\"I\"], label=\"I\", color=\"red\")\n",
    "ax.plot(observed[\"t\"], observed[\"R\"], label=\"R\", color=\"green\")\n",
    "ax.set_xlabel(\"Time (days)\")\n",
    "ax.set_ylabel(\"Population\")\n",
    "ax.set_title(f\"Synthetic Epidemic (β = {BETA_TRUE})\")\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Define the Log-Posterior\n",
    "\n",
    "For Bayesian inference, we need to define:\n",
    "\n",
    "1. **Likelihood**: How probable is the observed data given parameters?\n",
    "2. **Prior**: What do we believe about parameters before seeing data?\n",
    "\n",
    "The posterior is proportional to: `posterior ∝ likelihood × prior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration (fixed during inference)\n",
    "MODEL_KEY = jax.random.PRNGKey(0)  # Fixed for reproducibility\n",
    "\n",
    "def log_likelihood(beta):\n",
    "    \"\"\"Gaussian log-likelihood for the infection curve.\"\"\"\n",
    "    pred = run_diff_sir(\n",
    "        N_agents=N_AGENTS,\n",
    "        I0=I0,\n",
    "        beta=beta,\n",
    "        gamma=GAMMA,\n",
    "        T=T,\n",
    "        config=DiffConfig(tau=0.5, hard=True),\n",
    "        key=MODEL_KEY,\n",
    "    )\n",
    "    \n",
    "    # Gaussian likelihood with sigma = 10 (observation noise)\n",
    "    # Using larger sigma to smooth the likelihood surface\n",
    "    sigma = 10.0\n",
    "    residuals = pred[\"I\"] - observed_I\n",
    "    return -0.5 * jnp.sum((residuals / sigma) ** 2)\n",
    "\n",
    "\n",
    "def log_prior(beta):\n",
    "    \"\"\"Log-prior: beta ~ Normal(0.3, 0.15), constrained to [0, 1].\"\"\"\n",
    "    # Gaussian prior centered at 0.3\n",
    "    prior_mean = 0.3\n",
    "    prior_std = 0.15\n",
    "    log_p = -0.5 * ((beta - prior_mean) / prior_std) ** 2\n",
    "    \n",
    "    # Soft constraint to keep beta in reasonable range\n",
    "    # (penalize values outside [0.05, 0.8])\n",
    "    penalty = jnp.where(beta < 0.05, -100 * (0.05 - beta) ** 2, 0.0)\n",
    "    penalty += jnp.where(beta > 0.8, -100 * (beta - 0.8) ** 2, 0.0)\n",
    "    \n",
    "    return log_p + penalty\n",
    "\n",
    "\n",
    "def log_posterior(beta):\n",
    "    \"\"\"Log-posterior = log-likelihood + log-prior.\"\"\"\n",
    "    return log_likelihood(beta) + log_prior(beta)\n",
    "\n",
    "\n",
    "# Test the log-posterior\n",
    "test_beta = jnp.array(0.3)\n",
    "print(f\"Log-posterior at β=0.3: {log_posterior(test_beta):.2f}\")\n",
    "print(f\"Log-posterior at β=0.35 (true): {log_posterior(jnp.array(BETA_TRUE)):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run MCMC Sampling\n",
    "\n",
    "We'll use BlackJAX's NUTS (No-U-Turn Sampler) algorithm, which is an adaptive form of Hamiltonian Monte Carlo that automatically tunes the step size and trajectory length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_BLACKJAX:\n",
    "    # Initialize sampler\n",
    "    rng_key = jax.random.PRNGKey(123)\n",
    "    initial_position = jnp.array(0.30)  # Starting point\n",
    "    \n",
    "    # Set up NUTS with window adaptation for warmup\n",
    "    # Note: num_steps is passed to .run(), not the constructor (BlackJAX >= 1.0)\n",
    "    warmup = blackjax.window_adaptation(\n",
    "        blackjax.nuts,\n",
    "        log_posterior,\n",
    "        initial_step_size=0.001,  # Small initial step size for stability\n",
    "    )\n",
    "    \n",
    "    # Run warmup\n",
    "    print(\"Running warmup (this may take a minute)...\")\n",
    "    rng_key, warmup_key = jax.random.split(rng_key)\n",
    "    (state, params), _ = warmup.run(warmup_key, initial_position, num_steps=500)\n",
    "    print(f\"Adapted step size: {params['step_size']:.6f}\")\n",
    "    \n",
    "    # Set up sampling kernel\n",
    "    nuts_kernel = blackjax.nuts(log_posterior, **params).step\n",
    "    \n",
    "    # Sampling loop\n",
    "    def one_step(carry, rng_key):\n",
    "        state = carry\n",
    "        state, info = nuts_kernel(rng_key, state)\n",
    "        return state, state.position\n",
    "    \n",
    "    # Run sampling\n",
    "    print(\"Running sampling...\")\n",
    "    n_samples = 500\n",
    "    rng_key, sample_key = jax.random.split(rng_key)\n",
    "    keys = jax.random.split(sample_key, n_samples)\n",
    "    \n",
    "    final_state, samples = jax.lax.scan(one_step, state, keys)\n",
    "    samples = np.array(samples)\n",
    "    \n",
    "    print(f\"\\nCollected {len(samples)} samples\")\n",
    "    print(f\"Posterior mean: {samples.mean():.4f}\")\n",
    "    print(f\"Posterior std: {samples.std():.4f}\")\n",
    "    print(f\"True value: {BETA_TRUE}\")\n",
    "else:\n",
    "    print(\"Skipping MCMC - BlackJAX not installed\")\n",
    "    samples = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Analyze the Posterior\n",
    "\n",
    "Let's visualize the posterior distribution and check convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if samples is not None:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "    \n",
    "    # Trace plot\n",
    "    ax = axes[0]\n",
    "    ax.plot(samples, alpha=0.7, linewidth=0.5)\n",
    "    ax.axhline(y=BETA_TRUE, color=\"red\", linestyle=\"--\", label=f\"True β = {BETA_TRUE}\")\n",
    "    ax.set_xlabel(\"Sample\")\n",
    "    ax.set_ylabel(\"β\")\n",
    "    ax.set_title(\"Trace Plot\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # Histogram\n",
    "    ax = axes[1]\n",
    "    ax.hist(samples, bins=40, density=True, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\n",
    "    ax.axvline(x=BETA_TRUE, color=\"red\", linestyle=\"--\", linewidth=2, label=f\"True β = {BETA_TRUE}\")\n",
    "    ax.axvline(x=samples.mean(), color=\"green\", linestyle=\"-\", linewidth=2, label=f\"Mean = {samples.mean():.3f}\")\n",
    "    ax.set_xlabel(\"β\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.set_title(\"Posterior Distribution\")\n",
    "    ax.legend()\n",
    "    \n",
    "    # Autocorrelation\n",
    "    ax = axes[2]\n",
    "    max_lag = 50\n",
    "    acf = [np.corrcoef(samples[:-lag], samples[lag:])[0, 1] if lag > 0 else 1.0 for lag in range(max_lag)]\n",
    "    ax.bar(range(max_lag), acf, color=\"steelblue\", alpha=0.7)\n",
    "    ax.axhline(y=0, color=\"black\", linewidth=0.5)\n",
    "    ax.set_xlabel(\"Lag\")\n",
    "    ax.set_ylabel(\"Autocorrelation\")\n",
    "    ax.set_title(\"Autocorrelation Function\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nPosterior Summary:\")\n",
    "    print(f\"  Mean: {samples.mean():.4f}\")\n",
    "    print(f\"  Std:  {samples.std():.4f}\")\n",
    "    print(f\"  2.5%: {np.percentile(samples, 2.5):.4f}\")\n",
    "    print(f\"  97.5%: {np.percentile(samples, 97.5):.4f}\")\n",
    "    print(f\"  True:  {BETA_TRUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Posterior Predictive Check\n",
    "\n",
    "We can use samples from the posterior to generate predictions and visualize uncertainty in the model fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if samples is not None:\n",
    "    # Generate predictions from posterior samples\n",
    "    n_pred = 100  # Number of posterior samples to use\n",
    "    sample_indices = np.random.choice(len(samples), n_pred, replace=False)\n",
    "    \n",
    "    predictions = []\n",
    "    for idx in sample_indices:\n",
    "        beta_sample = samples[idx]\n",
    "        pred = run_diff_sir(\n",
    "            N_agents=N_AGENTS,\n",
    "            I0=I0,\n",
    "            beta=beta_sample,\n",
    "            gamma=GAMMA,\n",
    "            T=T,\n",
    "            config=DiffConfig(tau=0.5, hard=True),\n",
    "            key=MODEL_KEY,\n",
    "        )\n",
    "        predictions.append(np.array(pred[\"I\"]))\n",
    "    \n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Posterior predictive interval\n",
    "    pred_mean = predictions.mean(axis=0)\n",
    "    pred_lower = np.percentile(predictions, 2.5, axis=0)\n",
    "    pred_upper = np.percentile(predictions, 97.5, axis=0)\n",
    "    \n",
    "    t = np.array(observed[\"t\"])\n",
    "    ax.fill_between(t, pred_lower, pred_upper, alpha=0.3, color=\"steelblue\", label=\"95% CI\")\n",
    "    ax.plot(t, pred_mean, color=\"steelblue\", linewidth=2, label=\"Posterior mean\")\n",
    "    ax.scatter(t, np.array(observed_I), color=\"black\", s=30, zorder=5, label=\"Observed\")\n",
    "    \n",
    "    ax.set_xlabel(\"Time (days)\", fontsize=12)\n",
    "    ax.set_ylabel(\"Number Infected\", fontsize=12)\n",
    "    ax.set_title(\"Posterior Predictive Check\", fontsize=14)\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial, we demonstrated:\n",
    "\n",
    "1. **Differentiable likelihood**: Using `run_diff_sir` to compute gradients through the epidemic model\n",
    "2. **Bayesian inference**: Combining likelihood with priors to form a posterior\n",
    "3. **MCMC sampling**: Using BlackJAX NUTS to sample from the posterior\n",
    "4. **Posterior analysis**: Visualizing uncertainty and performing predictive checks\n",
    "\n",
    "The key advantage of differentiable models is that HMC/NUTS can use gradient information to explore the posterior efficiently, which is especially important for:\n",
    "\n",
    "- **High-dimensional problems**: Many parameters to estimate\n",
    "- **Complex posteriors**: Multi-modal or correlated parameters\n",
    "- **Expensive models**: Where efficient exploration is crucial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
